{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bd515c",
   "metadata": {},
   "source": [
    "### Frame the Problem and Look at the Big Picture \n",
    "\n",
    "- Define the objective in business terms \n",
    "- How will our solution be used \n",
    "- What are the current solutions / workarounds, if any\n",
    "- How should we frame the problem (supervised/usupervised, online/offline)\n",
    "- How should performance be measured \n",
    "- Is the perforamnce measure aligned with the business objective\n",
    "- What is the performance needed to achieve the business objective \n",
    "- What are comparable problems? Can we reuse experience or tools\n",
    "- Is human expertise avaiable \n",
    "- List the assumptions \n",
    "- Verify assumptions \n",
    "\n",
    "### Get the Data \n",
    "\n",
    "- List the data we need and how much we need\n",
    "- Find and document where you can get the data\n",
    "- Check how much space it will take\n",
    "- Check legal obligations \n",
    "- Get access authorisations\n",
    "- Create workspace\n",
    "- Get the data\n",
    "- Covert the data into a format we can easily work with \n",
    "- Ensure sensitivie information if deleted or protected \n",
    "- Check size and type of data (time series, sample, geographical) \n",
    "- Sample a test set, put it aside and never look at it!\n",
    "\n",
    "### Explore the Data \n",
    "\n",
    "- Create a copy of the data for exploration \n",
    "- Create a jupyer notebook to keep a record of our data exploration \n",
    "- Study each attribute and its characteristics \n",
    "    - name \n",
    "    - type (catagorical, int/float, bounded/unbounded, text, structured etc) \n",
    "    - % of missing values \n",
    "    - Noisiness and type of noise (stachastic, outliers, rounding errors etc)\n",
    "    - Usefulness for the task \n",
    "    - Types of distribution (Gaussian, uniform, logarithm etc) \n",
    "- Identify the target attributes (supervised learning) \n",
    "- Visualise the data\n",
    "- Study correlations in the data\n",
    "- Study how we would solve the problem manually \n",
    "- Identify the promising transformations we may want to apply \n",
    "- Identify exta data that would be useful\n",
    "- Document what we have learned \n",
    "\n",
    "### Prepare the Data\n",
    "\n",
    "- Work on copies of the data (keep the original dataset intact)\n",
    "- Write functions for all of the data transforamtions we want to apply\n",
    "    - So we can easliy prep the data for the next time we get a fresh dataset\n",
    "    - So we can apply these transformations to future projects\n",
    "    - To clean and prepare the test dataset\n",
    "    - To clean and prapre the new data instances once our solution is live \n",
    "    - To make it easy to treat your preperation choices as hyperparameters\n",
    "- Data cleaning \n",
    "    - Remove or fix outliers\n",
    "    - Fill in missing coloumns \n",
    "- Feature extraction \n",
    "    - Drop the attributes that provide no useful information to the task\n",
    "- Feature engineering \n",
    "    - Discretize continuous factors \n",
    "    - Decompose features (catagorical, data/time, etc) \n",
    "    - Add promising transformations of features (log(x), sqrt(x), x squared, etc) \n",
    "    - Aggregate features into promising new features \n",
    "- Feature scaling \n",
    "    - Standardise and normalise features\n",
    "  \n",
    "### Shortlist Promising Models\n",
    "\n",
    "- If the data is huge, we may want to sample into smaller training sets so we can train many different models in a respectable time\n",
    "- Try to automate these steps\n",
    "- Train many quick and dirty models from differnt catagories, \n",
    "- Measure and compare their perforamance \n",
    "- For each model use n-fold cross validation, and compute the mean and standard devisation for the perforamance measure on the N folds \n",
    "- Analyse the most significant variables for the algorithm \n",
    "- Analyse the errors that are being made \n",
    "    - What would a human do to avoid these errors \n",
    "- Perform a quick round of feature selection and engineering \n",
    "- perform one or two more quick iterations of the 5 previous steps \n",
    "- Shortlist the top 3-5 models, preferring models that make different types of errors \n",
    "\n",
    "### Fine-Tune the System \n",
    "\n",
    "- We will want to use as much data as possible for this step, especially as you move towards the end of fine tuning \n",
    "- Always automatic what we can \n",
    "- Fine tune the hyperparameters using cross-validation \n",
    "    - Treat your data transformation choices as hyperparameters especially when you are not sure about using them. (eg if we are not sure to replace the missing values or to just drop them) \n",
    "    - Unless there are very few hyperparameter values to explore, prefer random searches over grid searches \n",
    "- Try ensemble methods, combining our best models will often produce better perforamnce \n",
    "- Once you are confident about our final model measure its performance on the test set to look for its generalisation error value \n",
    "\n",
    "### Present your System \n",
    "\n",
    "- Document what we have done \n",
    "- Create a presentation, make sure we highlight the big picture first \n",
    "- Explain why our solution achieves the objective \n",
    "- Don't forget to present the interesting points we noticed along the way \n",
    "- Describe what worked and what did not \n",
    "- List your assumptions and our systems limitations \n",
    "- Ensure the key findings are communictaed throuhg visualisations or easy to remeber statements (median income is the number one dictor of housing prices) \n",
    "\n",
    "\n",
    "### Launch\n",
    "\n",
    "- Get your solution ready for production (plug into the production data inputs, write unit tests, etc.) \n",
    "- Write monitoring code that checks the system's live performance at regular intervals and triggers and alert when it dips. \n",
    "- Beware of slow degredation (rot) \n",
    "- Measuring performance may required a human pipeline \n",
    "- Alos monitors your inputs quality \n",
    "- Retrain your models on a regualar basis on fresh data (automate as much as possible) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
