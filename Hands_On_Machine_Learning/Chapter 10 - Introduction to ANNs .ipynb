{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Deep MLP on the MNIST Dataset and see if we can get over 98% precision. \n",
    "\n",
    "- Try searching for optimal learning rate by growing it exponentially, plotting the loss and finding the point where the loss shoots up.\n",
    "\n",
    "- Try adding saves, checkpoints and plotting the learning curves using TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import time\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data\n",
    "(X_train_complete, Y_train_complete), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# create the validation set\n",
    "X_train, X_valid = X_train_complete[5000:] / 250, X_train_complete[:5000] / 250 # Gradient descent requires that we scale. \n",
    "Y_train, Y_valid = Y_train_complete[5000:], Y_train_complete[:5000]\n",
    "\n",
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building a standard_model using the sequential API\n",
    "standard_model = keras.models.Sequential()\n",
    "\n",
    "standard_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "standard_model.add(keras.layers.Dense(300, activation='relu'))\n",
    "standard_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "standard_model.add(keras.layers.Dense(10, activation='softmax')) # Softmax as we are classifying, 10 because there are 10 classes. \n",
    "\n",
    "standard_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the weights and biases.\n",
    "\n",
    "first_hidden = standard_model.layers[1]\n",
    "weights, biases = first_hidden.get_weights()\n",
    "weights.shape\n",
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model.\n",
    "\n",
    "standard_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=[keras.metrics.SparseCategoricalAccuracy()]) # Using sparse categorical cross entropy as we are using integer labels not one hot encoding labels. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Callbacks and TensorBoard Log Directories for Training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard associated code. \n",
    "root_log_dir = os.path.join(os.curdir, \"logs\")\n",
    "\n",
    "# Create a specific sub directory to store the run data\n",
    "def current_run_log_dir():\n",
    "    '''Creates a current run directory'''\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_log_dir, run_id)\n",
    "\n",
    "\n",
    "my_logs = current_run_log_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the callbacks\n",
    "saver_cb = keras.callbacks.ModelCheckpoint(\"standard_mnist_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "tensor_board_cb = keras.callbacks.TensorBoard(my_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5982 - sparse_categorical_accuracy: 0.8442 - val_loss: 0.3076 - val_sparse_categorical_accuracy: 0.9148\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2865 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.2401 - val_sparse_categorical_accuracy: 0.9322\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2354 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.2065 - val_sparse_categorical_accuracy: 0.9404\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2020 - sparse_categorical_accuracy: 0.9419 - val_loss: 0.1774 - val_sparse_categorical_accuracy: 0.9520\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1765 - sparse_categorical_accuracy: 0.9495 - val_loss: 0.1587 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1563 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.1466 - val_sparse_categorical_accuracy: 0.9614\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1404 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.1335 - val_sparse_categorical_accuracy: 0.9612\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1269 - sparse_categorical_accuracy: 0.9638 - val_loss: 0.1251 - val_sparse_categorical_accuracy: 0.9654\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1151 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.1159 - val_sparse_categorical_accuracy: 0.9668\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1051 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.1094 - val_sparse_categorical_accuracy: 0.9692\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0963 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.1022 - val_sparse_categorical_accuracy: 0.9732\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0891 - sparse_categorical_accuracy: 0.9746 - val_loss: 0.0980 - val_sparse_categorical_accuracy: 0.9730\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0821 - sparse_categorical_accuracy: 0.9771 - val_loss: 0.0970 - val_sparse_categorical_accuracy: 0.9716\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0763 - sparse_categorical_accuracy: 0.9784 - val_loss: 0.0898 - val_sparse_categorical_accuracy: 0.9750\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0878 - val_sparse_categorical_accuracy: 0.9746\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0659 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0825 - val_sparse_categorical_accuracy: 0.9780\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0611 - sparse_categorical_accuracy: 0.9833 - val_loss: 0.0800 - val_sparse_categorical_accuracy: 0.9776\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0571 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.0814 - val_sparse_categorical_accuracy: 0.9774\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0538 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0777 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9866 - val_loss: 0.0769 - val_sparse_categorical_accuracy: 0.9788\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0474 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.0745 - val_sparse_categorical_accuracy: 0.9784\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0442 - sparse_categorical_accuracy: 0.9887 - val_loss: 0.0725 - val_sparse_categorical_accuracy: 0.9796\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.0734 - val_sparse_categorical_accuracy: 0.9800\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9901 - val_loss: 0.0714 - val_sparse_categorical_accuracy: 0.9800\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.0705 - val_sparse_categorical_accuracy: 0.9804\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0346 - sparse_categorical_accuracy: 0.9918 - val_loss: 0.0692 - val_sparse_categorical_accuracy: 0.9804\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.0700 - val_sparse_categorical_accuracy: 0.9794\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0308 - sparse_categorical_accuracy: 0.9927 - val_loss: 0.0663 - val_sparse_categorical_accuracy: 0.9810\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.0687 - val_sparse_categorical_accuracy: 0.9800\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0649 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0259 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0654 - val_sparse_categorical_accuracy: 0.9804\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9949 - val_loss: 0.0664 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0646 - val_sparse_categorical_accuracy: 0.9804\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0220 - sparse_categorical_accuracy: 0.9957 - val_loss: 0.0651 - val_sparse_categorical_accuracy: 0.9804\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.0657 - val_sparse_categorical_accuracy: 0.9808\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.0663 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9967 - val_loss: 0.0636 - val_sparse_categorical_accuracy: 0.9816\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9971 - val_loss: 0.0666 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.0641 - val_sparse_categorical_accuracy: 0.9820\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.0643 - val_sparse_categorical_accuracy: 0.9824\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0644 - val_sparse_categorical_accuracy: 0.9812\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.0642 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0136 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0650 - val_sparse_categorical_accuracy: 0.9824\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0638 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0647 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0647 - val_sparse_categorical_accuracy: 0.9814\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0647 - val_sparse_categorical_accuracy: 0.9820\n"
     ]
    }
   ],
   "source": [
    "history = standard_model.fit(X_train, Y_train, epochs=100, validation_data=(X_valid, Y_valid), callbacks=[saver_cb, early_stopping_cb, tensor_board_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4008), started 1:14:46 ago. (Use '!kill 4008' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-89deee1850844f42\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-89deee1850844f42\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating and connecting to the server\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 548us/step - loss: 13.4880 - sparse_categorical_accuracy: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.487970352172852, 0.9753000140190125]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_model = keras.models.load_model(\"standard_mnist_model.h5\")\n",
    "\n",
    "standard_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our standard model, we got an accuracy of 97%. Lets go and tune our model to improve this score to above 98%. There are several things we can tune with deep neural networks. \n",
    "\n",
    "- The number of layers.\n",
    "- The number of nodes in a layer. \n",
    "- The learning rate. \n",
    "- The batch size. \n",
    "- The optimizer.\n",
    "- The activation function. \n",
    "- The initialized weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering The Learning Rate\n",
    "\n",
    "- The standard LearningRateScheduler does not allow us to change when we implement our learning rate changes so we are going to use the custom one from the workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateIncreaser(keras.callbacks.Callback):\n",
    "    ''' Increases the learning rate by a %.'''\n",
    "    \n",
    "    def __init__(self, increase):\n",
    "        self.increase = increase\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        ''' Stores the current learning rate and loss, sets the new learning rate.'''\n",
    "        self.learning_rates.append(keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs['loss'])\n",
    "        keras.backend.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.increase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating another model where we will tune the learning rate. \n",
    "\n",
    "lr_tuned_model = keras.models.Sequential()\n",
    "\n",
    "lr_tuned_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "lr_tuned_model.add(keras.layers.Dense(200, activation='relu'))\n",
    "lr_tuned_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "lr_tuned_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "lr_tuned_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3), metrics=['accuracy']) # Starting really small. \n",
    "lr_tuner_cb = LearningRateIncreaser(increase=1.005) # Increasing by 0.5% each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 1ms/step - loss: nan - accuracy: 0.0990 - val_loss: nan - val_accuracy: 0.0958\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LearningRateIncreaser' object has no attribute 'rates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m history \u001b[38;5;241m=\u001b[39m lr_tuned_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_valid, Y_valid), callbacks\u001b[38;5;241m=\u001b[39m[lr_tuner_cb])\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlr_tuner_cb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrates\u001b[49m, lr_tuner_cb\u001b[38;5;241m.\u001b[39mlosses)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mgca()\u001b[38;5;241m.\u001b[39mset_xscale(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mhlines(\u001b[38;5;28mmin\u001b[39m(lr_tuner_cb\u001b[38;5;241m.\u001b[39mlosses), \u001b[38;5;28mmin\u001b[39m(lr_tuner_cb\u001b[38;5;241m.\u001b[39mrates), \u001b[38;5;28mmax\u001b[39m(lr_tuner_cb\u001b[38;5;241m.\u001b[39mrates))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LearningRateIncreaser' object has no attribute 'rates'"
     ]
    }
   ],
   "source": [
    "history = lr_tuned_model.fit(X_train, Y_train, epochs=1, validation_data=(X_valid, Y_valid), callbacks=[lr_tuner_cb])\n",
    "\n",
    "plt.plot(lr_tuner_cb.learning_rates, lr_tuner_cb.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(lr_tuner_cb.losses), min(lr_tuner_cb.rates), max(lr_tuner_cb.learning_rates))\n",
    "plt.axis([min(lr_tuner_cb.learning_rates), max(lr_tuner_cb.rates), 0, lr_tuner_cb.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24675d35c5a11b6e559b5e4fec28e1143ef6d3d4416ac15b11290f20a99bf8ea"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
