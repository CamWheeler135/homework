{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Deep MLP on the MNIST Dataset and see if we can get over 98% precision. \n",
    "\n",
    "- Try searching for optimal learning rate by growing it exponentially, plotting the loss and finding the point where the loss shoots up.\n",
    "\n",
    "- Try adding saves, checkpoints and plotting the learning curves using TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import time\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data\n",
    "(X_train_complete, Y_train_complete), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# create the validation set\n",
    "X_valid, X_train = X_train_complete[:5000] / 255., X_train_complete[5000:] / 255.\n",
    "Y_valid, Y_train = Y_train_complete[:5000], Y_train_complete[5000:]\n",
    "X_test = X_test / 255. # Always ensure that we scale our test data. \n",
    "\n",
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               157000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,110\n",
      "Trainable params: 178,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building a standard_model using the sequential API\n",
    "standard_model = keras.models.Sequential()\n",
    "\n",
    "standard_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "standard_model.add(keras.layers.Dense(200, activation='relu'))\n",
    "standard_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "standard_model.add(keras.layers.Dense(10, activation='softmax')) # Softmax as we are classifying, 10 because there are 10 classes. \n",
    "\n",
    "standard_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the weights and biases.\n",
    "\n",
    "first_hidden = standard_model.layers[1]\n",
    "weights, biases = first_hidden.get_weights()\n",
    "# weights.shape\n",
    "# biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model.\n",
    "\n",
    "standard_model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=[keras.metrics.SparseCategoricalAccuracy()]) # Using sparse categorical cross entropy as we are using integer labels not one hot encoding labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Callbacks and TensorBoard Log Directories for Training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard associated code. \n",
    "root_log_dir = os.path.join(os.curdir, \"logs\")\n",
    "\n",
    "# Create a specific sub directory to store the run data\n",
    "def current_run_log_dir():\n",
    "    '''Creates a current run directory'''\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_log_dir, run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the callbacks\n",
    "saver_cb = keras.callbacks.ModelCheckpoint(\"standard_mnist_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "my_logs = current_run_log_dir()\n",
    "tensor_board_cb = keras.callbacks.TensorBoard(my_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1719 [..............................] - ETA: 5:27 - loss: 2.2946 - sparse_categorical_accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 21:21:37.751007: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 979us/step - loss: 0.6416 - sparse_categorical_accuracy: 0.8302 - val_loss: 0.3155 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 2s 915us/step - loss: 0.2948 - sparse_categorical_accuracy: 0.9154 - val_loss: 0.2432 - val_sparse_categorical_accuracy: 0.9346\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 2s 946us/step - loss: 0.2411 - sparse_categorical_accuracy: 0.9314 - val_loss: 0.2077 - val_sparse_categorical_accuracy: 0.9444\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 2s 914us/step - loss: 0.2064 - sparse_categorical_accuracy: 0.9409 - val_loss: 0.1819 - val_sparse_categorical_accuracy: 0.9504\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 2s 921us/step - loss: 0.1806 - sparse_categorical_accuracy: 0.9477 - val_loss: 0.1602 - val_sparse_categorical_accuracy: 0.9558\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1598 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.1493 - val_sparse_categorical_accuracy: 0.9604\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.1435 - sparse_categorical_accuracy: 0.9584 - val_loss: 0.1339 - val_sparse_categorical_accuracy: 0.9620\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.1299 - sparse_categorical_accuracy: 0.9635 - val_loss: 0.1256 - val_sparse_categorical_accuracy: 0.9654\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.1182 - sparse_categorical_accuracy: 0.9666 - val_loss: 0.1198 - val_sparse_categorical_accuracy: 0.9670\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 2s 941us/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9687 - val_loss: 0.1095 - val_sparse_categorical_accuracy: 0.9706\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 2s 919us/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.1035 - val_sparse_categorical_accuracy: 0.9712\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 2s 922us/step - loss: 0.0932 - sparse_categorical_accuracy: 0.9734 - val_loss: 0.1015 - val_sparse_categorical_accuracy: 0.9710\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 2s 917us/step - loss: 0.0864 - sparse_categorical_accuracy: 0.9758 - val_loss: 0.0969 - val_sparse_categorical_accuracy: 0.9728\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 2s 935us/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9777 - val_loss: 0.0922 - val_sparse_categorical_accuracy: 0.9748\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 2s 951us/step - loss: 0.0751 - sparse_categorical_accuracy: 0.9791 - val_loss: 0.0902 - val_sparse_categorical_accuracy: 0.9740\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 2s 966us/step - loss: 0.0705 - sparse_categorical_accuracy: 0.9799 - val_loss: 0.0857 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 2s 986us/step - loss: 0.0660 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.0848 - val_sparse_categorical_accuracy: 0.9746\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 2s 961us/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9826 - val_loss: 0.0848 - val_sparse_categorical_accuracy: 0.9748\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 2s 967us/step - loss: 0.0583 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0865 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 2s 948us/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.0782 - val_sparse_categorical_accuracy: 0.9758\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 2s 944us/step - loss: 0.0519 - sparse_categorical_accuracy: 0.9859 - val_loss: 0.0775 - val_sparse_categorical_accuracy: 0.9760\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 2s 937us/step - loss: 0.0487 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.0771 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 2s 935us/step - loss: 0.0462 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.0742 - val_sparse_categorical_accuracy: 0.9762\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 2s 990us/step - loss: 0.0435 - sparse_categorical_accuracy: 0.9885 - val_loss: 0.0740 - val_sparse_categorical_accuracy: 0.9760\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 2s 997us/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9893 - val_loss: 0.0767 - val_sparse_categorical_accuracy: 0.9754\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 2s 926us/step - loss: 0.0389 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0745 - val_sparse_categorical_accuracy: 0.9774\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 2s 962us/step - loss: 0.0370 - sparse_categorical_accuracy: 0.9905 - val_loss: 0.0743 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9911 - val_loss: 0.0734 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 2s 931us/step - loss: 0.0333 - sparse_categorical_accuracy: 0.9916 - val_loss: 0.0716 - val_sparse_categorical_accuracy: 0.9784\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 2s 945us/step - loss: 0.0314 - sparse_categorical_accuracy: 0.9921 - val_loss: 0.0738 - val_sparse_categorical_accuracy: 0.9766\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 2s 978us/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.0756 - val_sparse_categorical_accuracy: 0.9764\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 2s 985us/step - loss: 0.0284 - sparse_categorical_accuracy: 0.9926 - val_loss: 0.0723 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 2s 914us/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9936 - val_loss: 0.0717 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9942 - val_loss: 0.0720 - val_sparse_categorical_accuracy: 0.9768\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 2s 951us/step - loss: 0.0240 - sparse_categorical_accuracy: 0.9946 - val_loss: 0.0705 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 2s 969us/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.0695 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 2s 974us/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9951 - val_loss: 0.0709 - val_sparse_categorical_accuracy: 0.9782\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.0713 - val_sparse_categorical_accuracy: 0.9782\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 2s 954us/step - loss: 0.0198 - sparse_categorical_accuracy: 0.9963 - val_loss: 0.0694 - val_sparse_categorical_accuracy: 0.9788\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 2s 905us/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0719 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 2s 912us/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9970 - val_loss: 0.0726 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 2s 981us/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.0700 - val_sparse_categorical_accuracy: 0.9788\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 2s 954us/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.0684 - val_sparse_categorical_accuracy: 0.9800\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0720 - val_sparse_categorical_accuracy: 0.9776\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.0688 - val_sparse_categorical_accuracy: 0.9788\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.0698 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 2s 931us/step - loss: 0.0135 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0740 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 2s 936us/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.0694 - val_sparse_categorical_accuracy: 0.9784\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 2s 991us/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0705 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 50/100\n",
      "1719/1719 [==============================] - 2s 914us/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0692 - val_sparse_categorical_accuracy: 0.9798\n",
      "Epoch 51/100\n",
      "1719/1719 [==============================] - 2s 929us/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.0725 - val_sparse_categorical_accuracy: 0.9778\n",
      "Epoch 52/100\n",
      "1719/1719 [==============================] - 2s 911us/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9989 - val_loss: 0.0698 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 53/100\n",
      "1719/1719 [==============================] - 2s 911us/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9990 - val_loss: 0.0702 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 54/100\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0708 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 55/100\n",
      "1719/1719 [==============================] - 2s 927us/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0724 - val_sparse_categorical_accuracy: 0.9784\n",
      "Epoch 56/100\n",
      "1719/1719 [==============================] - 2s 906us/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0718 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 57/100\n",
      "1719/1719 [==============================] - 2s 913us/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0719 - val_sparse_categorical_accuracy: 0.9780\n",
      "Epoch 58/100\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9994 - val_loss: 0.0704 - val_sparse_categorical_accuracy: 0.9796\n",
      "Epoch 59/100\n",
      "1719/1719 [==============================] - 2s 932us/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0711 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 60/100\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0711 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 61/100\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0717 - val_sparse_categorical_accuracy: 0.9784\n",
      "Epoch 62/100\n",
      "1719/1719 [==============================] - 2s 916us/step - loss: 0.0072 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.0713 - val_sparse_categorical_accuracy: 0.9788\n",
      "Epoch 63/100\n",
      "1719/1719 [==============================] - 2s 988us/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.0707 - val_sparse_categorical_accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "history = standard_model.fit(X_train, Y_train, epochs=100, validation_data=(X_valid, Y_valid), callbacks=[saver_cb, early_stopping_cb, tensor_board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 496us/step - loss: 0.0660 - sparse_categorical_accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06598667055368423, 0.9794999957084656]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_model = keras.models.load_model(\"standard_mnist_model.h5\")\n",
    "\n",
    "standard_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our standard model, we got an accuracy of 97%. Lets go and tune our model to improve this score to above 98%. There are several things we can tune with deep neural networks. \n",
    "\n",
    "- The number of layers.\n",
    "- The number of nodes in a layer. \n",
    "- The learning rate. \n",
    "- The batch size. \n",
    "- The optimizer.\n",
    "- The activation function. \n",
    "- The initialized weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning The Learning Rate\n",
    "\n",
    "- The standard LearningRateScheduler does not allow us to change when we implement our learning rate changes so we are going to use the custom one from the workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateIncreaser(keras.callbacks.Callback):\n",
    "    ''' Increases the learning rate by a %.'''\n",
    "    \n",
    "    def __init__(self, increase):\n",
    "        self.increase = increase\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        ''' Stores the current learning rate and loss, sets the new learning rate.'''\n",
    "        self.learning_rates.append(keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs['loss'])\n",
    "        keras.backend.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.increase)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating another model where we will tune the learning rate. \n",
    "\n",
    "lr_tester_model = keras.models.Sequential()\n",
    "\n",
    "lr_tester_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "lr_tester_model.add(keras.layers.Dense(200, activation='relu'))\n",
    "lr_tester_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "lr_tester_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "lr_tester_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3), metrics=['accuracy']) # Starting really small. \n",
    "lr_tuner_cb = LearningRateIncreaser(increase=1.005) # Increasing by 0.5% each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 1ms/step - loss: 125463478272.0000 - accuracy: 0.5807 - val_loss: 2.3911 - val_accuracy: 0.1126\n",
      "5.2636895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = lr_tester_model.fit(X_train, Y_train, epochs=1, validation_data=(X_valid, Y_valid), callbacks=[lr_tuner_cb])\n",
    "\n",
    "print(max(lr_tuner_cb.learning_rates))\n",
    "\n",
    "plt.plot(lr_tuner_cb.learning_rates, lr_tuner_cb.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(lr_tuner_cb.losses), min(lr_tuner_cb.learning_rates), max(lr_tuner_cb.learning_rates))\n",
    "plt.axis([min(lr_tuner_cb.learning_rates), max(lr_tuner_cb.learning_rates), 0, lr_tuner_cb.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss shoots up rapidly around $8e^{-1}$, the rule of thumb is that the optimal learning rate is half of the maximum learning rate. So lets set our learning rate at $4e^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model and setting the tuned learning rate. \n",
    "\n",
    "lr_tuned_model = keras.models.Sequential()\n",
    "\n",
    "lr_tuned_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "lr_tuned_model.add(keras.layers.Dense(200, activation='relu'))\n",
    "lr_tuned_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "lr_tuned_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "lr_tuned_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=4e-1), metrics=['accuracy']) # Tuned learning rate, we have deduced this number as 1/2 of the max learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the callbacks\n",
    "saver_cb = keras.callbacks.ModelCheckpoint(\"lr_tuned_mnist_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "my_logs = current_run_log_dir()\n",
    "tensor_board_cb = keras.callbacks.TensorBoard(my_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2498 - accuracy: 0.9229 - val_loss: 0.1088 - val_accuracy: 0.9672\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 2s 914us/step - loss: 0.1050 - accuracy: 0.9677 - val_loss: 0.0936 - val_accuracy: 0.9728\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 2s 911us/step - loss: 0.0776 - accuracy: 0.9760 - val_loss: 0.0888 - val_accuracy: 0.9732\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 2s 901us/step - loss: 0.0580 - accuracy: 0.9817 - val_loss: 0.0940 - val_accuracy: 0.9758\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 2s 929us/step - loss: 0.0496 - accuracy: 0.9847 - val_loss: 0.0861 - val_accuracy: 0.9780\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.0410 - accuracy: 0.9865 - val_loss: 0.0862 - val_accuracy: 0.9786\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 2s 905us/step - loss: 0.0330 - accuracy: 0.9893 - val_loss: 0.0983 - val_accuracy: 0.9748\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.0236 - accuracy: 0.9925 - val_loss: 0.1002 - val_accuracy: 0.9766\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 2s 936us/step - loss: 0.0207 - accuracy: 0.9936 - val_loss: 0.0952 - val_accuracy: 0.9818\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 2s 906us/step - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.1300 - val_accuracy: 0.9752\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.0206 - accuracy: 0.9932 - val_loss: 0.1030 - val_accuracy: 0.9810\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 2s 904us/step - loss: 0.0179 - accuracy: 0.9942 - val_loss: 0.1101 - val_accuracy: 0.9778\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 2s 937us/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.1032 - val_accuracy: 0.9796\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 2s 906us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1286 - val_accuracy: 0.9742\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 2s 905us/step - loss: 0.0209 - accuracy: 0.9940 - val_loss: 0.1084 - val_accuracy: 0.9774\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 2s 998us/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.1029 - val_accuracy: 0.9812\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 2s 960us/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1249 - val_accuracy: 0.9778\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 2s 904us/step - loss: 0.0168 - accuracy: 0.9950 - val_loss: 0.1400 - val_accuracy: 0.9752\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 2s 906us/step - loss: 0.0176 - accuracy: 0.9950 - val_loss: 0.1736 - val_accuracy: 0.9718\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 2s 907us/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.1202 - val_accuracy: 0.9808\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 2s 921us/step - loss: 0.0154 - accuracy: 0.9955 - val_loss: 0.1327 - val_accuracy: 0.9796\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 2s 903us/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.1472 - val_accuracy: 0.9792\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.1517 - val_accuracy: 0.9796\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 2s 905us/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 0.1548 - val_accuracy: 0.9776\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 2s 930us/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.1386 - val_accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "history = lr_tuned_model.fit(X_train, Y_train, epochs=100, validation_data=(X_valid, Y_valid), callbacks=[saver_cb, early_stopping_cb, tensor_board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Tuned Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 501us/step - loss: 0.0851 - accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0850844755768776, 0.9757999777793884]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model with the tuned learning rate.\n",
    "\n",
    "lr_tuned_model = keras.models.load_model('lr_tuned_mnist_model.h5')\n",
    "\n",
    "lr_tuned_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning our learning rate gave us no gain in accuracy compared to the standard model, however there are still more things to tune. After reading the chapter, it seems like the number of layers is an important hyperparameter to look at, to put it like Aurlien \"You get more bang for your buck\" when you tune the layers. \n",
    "\n",
    "I am going to add one more layer to the network and see how it performs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Layer Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new model with an extra layer.\n",
    "\n",
    "layers_tuned_model = keras.models.Sequential()\n",
    "\n",
    "layers_tuned_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "layers_tuned_model.add(keras.layers.Dense(300, activation='relu'))\n",
    "layers_tuned_model.add(keras.layers.Dense(200, activation='relu'))\n",
    "layers_tuned_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "layers_tuned_model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "\n",
    "layers_tuned_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the callbacks\n",
    "\n",
    "saver_cb = keras.callbacks.ModelCheckpoint(\"layer_tuned_mnist_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "my_logs = current_run_log_dir()\n",
    "tensor_board_cb = keras.callbacks.TensorBoard(my_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.6283 - accuracy: 0.8307 - val_loss: 0.2845 - val_accuracy: 0.9216\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2668 - accuracy: 0.9229 - val_loss: 0.2187 - val_accuracy: 0.9372\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2089 - accuracy: 0.9393 - val_loss: 0.1702 - val_accuracy: 0.9520\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1717 - accuracy: 0.9500 - val_loss: 0.1467 - val_accuracy: 0.9584\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.1461 - accuracy: 0.9577 - val_loss: 0.1324 - val_accuracy: 0.9644\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1266 - accuracy: 0.9627 - val_loss: 0.1196 - val_accuracy: 0.9686\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1109 - accuracy: 0.9674 - val_loss: 0.1118 - val_accuracy: 0.9692\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0978 - accuracy: 0.9715 - val_loss: 0.1150 - val_accuracy: 0.9670\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0879 - accuracy: 0.9750 - val_loss: 0.0943 - val_accuracy: 0.9738\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0786 - accuracy: 0.9777 - val_loss: 0.0883 - val_accuracy: 0.9756\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0703 - accuracy: 0.9802 - val_loss: 0.0866 - val_accuracy: 0.9752\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0631 - accuracy: 0.9824 - val_loss: 0.0858 - val_accuracy: 0.9750\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0573 - accuracy: 0.9836 - val_loss: 0.0775 - val_accuracy: 0.9782\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0513 - accuracy: 0.9853 - val_loss: 0.0769 - val_accuracy: 0.9792\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0470 - accuracy: 0.9867 - val_loss: 0.0746 - val_accuracy: 0.9798\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0421 - accuracy: 0.9884 - val_loss: 0.0769 - val_accuracy: 0.9792\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0382 - accuracy: 0.9899 - val_loss: 0.0735 - val_accuracy: 0.9788\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0348 - accuracy: 0.9911 - val_loss: 0.0733 - val_accuracy: 0.9784\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0312 - accuracy: 0.9921 - val_loss: 0.0707 - val_accuracy: 0.9818\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0285 - accuracy: 0.9928 - val_loss: 0.0722 - val_accuracy: 0.9808\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0258 - accuracy: 0.9939 - val_loss: 0.0687 - val_accuracy: 0.9806\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0230 - accuracy: 0.9946 - val_loss: 0.0724 - val_accuracy: 0.9802\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.0706 - val_accuracy: 0.9802\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0188 - accuracy: 0.9963 - val_loss: 0.0716 - val_accuracy: 0.9810\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0171 - accuracy: 0.9967 - val_loss: 0.0701 - val_accuracy: 0.9814\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 0.0676 - val_accuracy: 0.9814\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0141 - accuracy: 0.9976 - val_loss: 0.0689 - val_accuracy: 0.9814\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.0126 - accuracy: 0.9980 - val_loss: 0.0711 - val_accuracy: 0.9808\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0713 - val_accuracy: 0.9818\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0699 - val_accuracy: 0.9808\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0094 - accuracy: 0.9989 - val_loss: 0.0737 - val_accuracy: 0.9802\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0085 - accuracy: 0.9991 - val_loss: 0.0708 - val_accuracy: 0.9812\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0078 - accuracy: 0.9992 - val_loss: 0.0711 - val_accuracy: 0.9816\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0071 - accuracy: 0.9993 - val_loss: 0.0745 - val_accuracy: 0.9818\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0065 - accuracy: 0.9994 - val_loss: 0.0734 - val_accuracy: 0.9816\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.0750 - val_accuracy: 0.9806\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9996 - val_loss: 0.0728 - val_accuracy: 0.9826\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0051 - accuracy: 0.9997 - val_loss: 0.0750 - val_accuracy: 0.9804\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 0.0743 - val_accuracy: 0.9808\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0043 - accuracy: 0.9998 - val_loss: 0.0739 - val_accuracy: 0.9816\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0040 - accuracy: 0.9998 - val_loss: 0.0747 - val_accuracy: 0.9822\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.0750 - val_accuracy: 0.9816\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0035 - accuracy: 0.9999 - val_loss: 0.0780 - val_accuracy: 0.9818\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.0771 - val_accuracy: 0.9816\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0030 - accuracy: 0.9999 - val_loss: 0.0772 - val_accuracy: 0.9814\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.0767 - val_accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "history = layers_tuned_model.fit(X_train, Y_train, epochs=100, validation_data=(X_valid, Y_valid), callbacks=[saver_cb, early_stopping_cb, tensor_board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Layers Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 626us/step - loss: 0.0764 - accuracy: 0.9781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07637351006269455, 0.9781000018119812]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_tuned_model = keras.models.load_model('layer_tuned_mnist_model.h5')\n",
    "\n",
    "layers_tuned_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight performance increase, lets look at the learning curves and see what is happening..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Number of Nodes\n",
    "\n",
    "- We have looked at tuning our learning rate and the number of layers in the network but it appears to not be affecting our performance well. \n",
    "\n",
    "- When comparing the standard model to the model with more layers, it appears that we start to overfit our training data when we have 3 layers, which causes training to cut short as we have early stopping. \n",
    "\n",
    "- Having thought about this, I am going to experiment with the number of nodes in the first layer, without altering the number of layers in the network. This might allow us to increase the complexity of the network, without increasing it so much that we over fit the data early (like in the network with 3 hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with more layers in the first layer of the network.\n",
    "\n",
    "node_tuned_model = keras.models.Sequential()\n",
    "\n",
    "node_tuned_model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "node_tuned_model.add(keras.layers.Dense(300, activation='relu'))\n",
    "node_tuned_model.add(keras.layers.Dense(100, activation='relu'))\n",
    "node_tuned_model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_tuned_model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(), metrics=['accuracy']) # Standard learning rate as that performed better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the callbacks\n",
    "saver_cb = keras.callbacks.ModelCheckpoint(\"node_tuned_mnist_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "\n",
    "my_logs = current_run_log_dir()\n",
    "tensor_board_cb = keras.callbacks.TensorBoard(my_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6215 - accuracy: 0.8427 - val_loss: 0.3067 - val_accuracy: 0.9142\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2939 - accuracy: 0.9157 - val_loss: 0.2436 - val_accuracy: 0.9310\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2415 - accuracy: 0.9307 - val_loss: 0.2033 - val_accuracy: 0.9440\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2064 - accuracy: 0.9408 - val_loss: 0.1803 - val_accuracy: 0.9516\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1805 - accuracy: 0.9486 - val_loss: 0.1605 - val_accuracy: 0.9546\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1600 - accuracy: 0.9543 - val_loss: 0.1478 - val_accuracy: 0.9606\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1432 - accuracy: 0.9600 - val_loss: 0.1352 - val_accuracy: 0.9638\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1296 - accuracy: 0.9629 - val_loss: 0.1283 - val_accuracy: 0.9652\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1178 - accuracy: 0.9671 - val_loss: 0.1200 - val_accuracy: 0.9656\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1080 - accuracy: 0.9692 - val_loss: 0.1094 - val_accuracy: 0.9690\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0992 - accuracy: 0.9721 - val_loss: 0.1069 - val_accuracy: 0.9704\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0913 - accuracy: 0.9743 - val_loss: 0.1003 - val_accuracy: 0.9716\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0845 - accuracy: 0.9759 - val_loss: 0.0963 - val_accuracy: 0.9734\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0782 - accuracy: 0.9780 - val_loss: 0.0919 - val_accuracy: 0.9740\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0727 - accuracy: 0.9798 - val_loss: 0.0878 - val_accuracy: 0.9748\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0678 - accuracy: 0.9811 - val_loss: 0.0858 - val_accuracy: 0.9754\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0633 - accuracy: 0.9826 - val_loss: 0.0834 - val_accuracy: 0.9764\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0593 - accuracy: 0.9841 - val_loss: 0.0803 - val_accuracy: 0.9766\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0553 - accuracy: 0.9847 - val_loss: 0.0795 - val_accuracy: 0.9760\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0519 - accuracy: 0.9863 - val_loss: 0.0791 - val_accuracy: 0.9766\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0487 - accuracy: 0.9872 - val_loss: 0.0744 - val_accuracy: 0.9790\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0458 - accuracy: 0.9882 - val_loss: 0.0751 - val_accuracy: 0.9790\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0430 - accuracy: 0.9891 - val_loss: 0.0743 - val_accuracy: 0.9774\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0405 - accuracy: 0.9898 - val_loss: 0.0731 - val_accuracy: 0.9782\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0383 - accuracy: 0.9905 - val_loss: 0.0700 - val_accuracy: 0.9792\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0359 - accuracy: 0.9912 - val_loss: 0.0696 - val_accuracy: 0.9802\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0341 - accuracy: 0.9918 - val_loss: 0.0678 - val_accuracy: 0.9804\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0322 - accuracy: 0.9923 - val_loss: 0.0705 - val_accuracy: 0.9786\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0301 - accuracy: 0.9930 - val_loss: 0.0684 - val_accuracy: 0.9796\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0286 - accuracy: 0.9932 - val_loss: 0.0682 - val_accuracy: 0.9798\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0270 - accuracy: 0.9942 - val_loss: 0.0677 - val_accuracy: 0.9812\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0254 - accuracy: 0.9945 - val_loss: 0.0670 - val_accuracy: 0.9802\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0241 - accuracy: 0.9948 - val_loss: 0.0659 - val_accuracy: 0.9812\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0228 - accuracy: 0.9954 - val_loss: 0.0687 - val_accuracy: 0.9798\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0214 - accuracy: 0.9959 - val_loss: 0.0658 - val_accuracy: 0.9806\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0202 - accuracy: 0.9961 - val_loss: 0.0687 - val_accuracy: 0.9802\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0192 - accuracy: 0.9967 - val_loss: 0.0654 - val_accuracy: 0.9814\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 0.0662 - val_accuracy: 0.9810\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.0667 - val_accuracy: 0.9802\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0164 - accuracy: 0.9976 - val_loss: 0.0659 - val_accuracy: 0.9804\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0157 - accuracy: 0.9976 - val_loss: 0.0647 - val_accuracy: 0.9808\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0148 - accuracy: 0.9980 - val_loss: 0.0669 - val_accuracy: 0.9804\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0142 - accuracy: 0.9980 - val_loss: 0.0685 - val_accuracy: 0.9800\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0134 - accuracy: 0.9983 - val_loss: 0.0651 - val_accuracy: 0.9822\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.0658 - val_accuracy: 0.9818\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.0659 - val_accuracy: 0.9802\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0115 - accuracy: 0.9987 - val_loss: 0.0657 - val_accuracy: 0.9812\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0109 - accuracy: 0.9989 - val_loss: 0.0676 - val_accuracy: 0.9806\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0103 - accuracy: 0.9991 - val_loss: 0.0647 - val_accuracy: 0.9810\n",
      "Epoch 50/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0099 - accuracy: 0.9990 - val_loss: 0.0667 - val_accuracy: 0.9812\n",
      "Epoch 51/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0095 - accuracy: 0.9991 - val_loss: 0.0663 - val_accuracy: 0.9814\n",
      "Epoch 52/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0091 - accuracy: 0.9992 - val_loss: 0.0653 - val_accuracy: 0.9816\n",
      "Epoch 53/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0086 - accuracy: 0.9993 - val_loss: 0.0660 - val_accuracy: 0.9810\n",
      "Epoch 54/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0083 - accuracy: 0.9993 - val_loss: 0.0661 - val_accuracy: 0.9822\n",
      "Epoch 55/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0079 - accuracy: 0.9993 - val_loss: 0.0664 - val_accuracy: 0.9810\n",
      "Epoch 56/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 0.0673 - val_accuracy: 0.9810\n",
      "Epoch 57/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0073 - accuracy: 0.9994 - val_loss: 0.0664 - val_accuracy: 0.9816\n",
      "Epoch 58/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0070 - accuracy: 0.9995 - val_loss: 0.0684 - val_accuracy: 0.9820\n",
      "Epoch 59/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9995 - val_loss: 0.0669 - val_accuracy: 0.9820\n",
      "Epoch 60/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0064 - accuracy: 0.9996 - val_loss: 0.0678 - val_accuracy: 0.9818\n",
      "Epoch 61/100\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0062 - accuracy: 0.9997 - val_loss: 0.0674 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17511fc40>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_tuned_model.fit(X_train, Y_train, epochs=100, validation_data=(X_valid, Y_valid), callbacks=[saver_cb, early_stopping_cb, tensor_board_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Node Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 676us/step - loss: 0.0638 - accuracy: 0.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06384775042533875, 0.9800999760627747]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_tuned_model = keras.models.load_model(\"node_tuned_mnist_model.h5\")\n",
    "\n",
    "node_tuned_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There we are at 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although we tried to tweak the learning rate, it appears that there might be some error in my computations, as the default learning rate produced the highest score. \n",
    "\n",
    "- It could be worth exploring \"automatic\" hyperparameter tuners. \n",
    "\n",
    "- I learned that having only 200 nodes in the first layer was stopping my network from getting to 98%, although the gap is miniscule the network is able to extract a deeper representation of the data during training with 300 nodes in the first network. \n",
    "\n",
    "- However adding another layer into the network did not help the situation. This problem puzzles me at the moment as I was expecting the extra layer to allow the network to further represent the data, even with 300 nodes in the first layer, it appears that having that second layer reduces the accuracy of the network. However, I cannot explain it at the moment in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24675d35c5a11b6e559b5e4fec28e1143ef6d3d4416ac15b11290f20a99bf8ea"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
