{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76986b4",
   "metadata": {},
   "source": [
    "# Chapter 03 - Classification \n",
    "\n",
    "In the first chapter the author mentioned that the most common form of supervised learning tasks are regression (where we are prediciting continuous variables) and classification (prediciting classes). In the previous chapter we explored a regression task, where we looked at prediciting the house prices using several types of algorithms, such as linear regression, decision trees and random forests. In this chapter, we are going to look at classification tasks. \n",
    "\n",
    "### MNIST Dataset \n",
    "\n",
    "For this chapter we will be using the MNIST dataset, this is a dataset of 70 thousand labelled images of handwritten numbers. The set has been studied so much that it is often called the \"hello world\" of machine learning (thinking back to when I did my first hello world when I started to learn coding). Whenever people come up with new classification algorithms, the first thing they do is test it on the MNIST dataset to see how it fairs. Anyone who does machine learning or is learning machine learning has tackled or is going to tackle this dataset at some point!!!!\n",
    "\n",
    "Scikit-learn provides many helper funtions to download popular datasets such as the MNIST dataset, the following code fetches the dataset for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfe9963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml \n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame = False) # we need to set the as_frame = false as skikit learn has updated\n",
    "\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc223f",
   "metadata": {},
   "source": [
    "Datasets that are loaded in my scikit learn generally have a similar dictionary structure that include the following. \n",
    "\n",
    "- A DESCR key describing the dataset\n",
    "- A data key containing an array with one row per instance and one colum per feature \n",
    "- A target key that contains all of the labels for each of the instances\n",
    "\n",
    "Let's look at these arrays below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e5871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf439794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34804a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f39b5",
   "metadata": {},
   "source": [
    "There are 70 thousand iages and each image is made up of 784 features. This is because each image is 28 x 28 pixels and each feature is simply representing the pixels intensity from 0 (which is white) to 255 (which is black). Lets take a look at one of the digits on our dataset. All we need to do is grab an instances feature vector (we know what these are now) and reshape it into the 28 x 28 array, and thi display it using Matplotlibs imshow() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb0cd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "first_digit = X[0]\n",
    "\n",
    "first_digit_image = first_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(first_digit_image, cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c731bbc7",
   "metadata": {},
   "source": [
    "So its a five! Lets look at the first label so prove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8ca39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd638c0",
   "metadata": {},
   "source": [
    "Perfect it's a five, note that the label is a string. Most ML algorithms expect numbers to be fed into them lets cast y to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3b496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all of the strings into integers\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae61ad",
   "metadata": {},
   "source": [
    "Remember that before we go onto analyse the dataset we need to create a test set. The MNIST dataset is actually already split into a training and test split (the first 60,000 are training and the last 10,000 is testing) so lets create our test train split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac367374",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[:60000], y[:60000], X[60000:], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da897a",
   "metadata": {},
   "source": [
    "It should be noted that the dataset is already shuffled which is good because it means that the cross validation folds should contain similar amount of each digit, as well as this machine learning algorithms don't tend to like being fed instances that are too similar one after eachother, shuffling the dataset ensures this wont happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8c9a9",
   "metadata": {},
   "source": [
    "### Training a Binary Classifer\n",
    "\n",
    "Let us go through a simpler problem for now, and simply try and identify one digit - for example the number 5 we have just looked at !!!! This \"5-detector\" lol will be an example of what is called a **binary classifer**, capable of distinguishing between just two classes (5 and not 5). Lets create the target vectors for the classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ccbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # setting the 5 labels to be the 5 training set\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac7f17",
   "metadata": {},
   "source": [
    "Now lets pick a classifier and train it. A good place to start is with a **stochastic gradient descent (SGD) classifier** using Scikitlearns SDGClassifer class. This has the advantage of being able to handle very large datasets efficiently. This is in part because the SGD classifier deals with training instances independently, one at a time (this also makes the SDG classifier really good for online learning) as we will see later. Lets create an SGD classifier and train it on the whole training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6baf0100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5) # Either it is a 5 or it isn't a 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccba20",
   "metadata": {},
   "source": [
    "NOTE: The stochastic gradient descent relies on randomness during training (hence the name \"stochastic\"). If we want repoducible results, we need to set the random seed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68cab36",
   "metadata": {},
   "source": [
    "Now lets use it to classify an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009eda6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.predict([first_digit]) # note that this is in [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90a73d",
   "metadata": {},
   "source": [
    "YAY, the classifier guesses that the image is a 5 (True). This looks like its guessed right in this case. Now let's evaluate this models performance overall.\n",
    "\n",
    "### Performance Measure\n",
    "\n",
    "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will be spending a large part of this chapter covering the this topic. There are many perfromance measures at our disposal, so grab a coffee and get ready to learn some new concepts and some new acronyms!!!!!!\n",
    "\n",
    "### Measuring Accuracy Using Cross-Validaton \n",
    "\n",
    "A great way to evaluate a model is to use cross validation, just like we did in chapter 2.\n",
    "\n",
    "However sometimes we will need more control over the cross validation process than what scikit-learn provides to us. In these cases, you can implement cross validation by hand. The following code does roughly the same thing as scikitlearns cross_val_score() function and it prints the same result. \n",
    "\n",
    "Lets take this oppourtunity to understand the inner mechanics of how the cross val score function work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6abd1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5): # splitting the data training set and label\n",
    "    clone_clf = clone(sgd_clf) # create a clone of the sgd classifier\n",
    "    X_train_folds = X_train[train_index] # assign the value to the x train set \n",
    "    y_train_folds = y_train_5[train_index] # assign the value to the y train set\n",
    "    X_test_fold = X_train[test_index] # assign the value to the x test set \n",
    "    y_test_fold = y_train_5[test_index] #assign the value to the y test set\n",
    "\n",
    "clone_clf.fit(X_train_folds, y_train_folds)\n",
    "y_prediction = clone_clf.predict(X_test_fold)\n",
    "number_correct = sum(y_prediction == y_test_fold)\n",
    "print(number_correct / len(y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543f1fb",
   "metadata": {},
   "source": [
    "The StratifiedKFold class performs stratified sampling to produce folds that contan a representative ratio of each class. At each iteration the code creates a clone of the classifier, trains that clone classifier on the training folds and makes predictions on that test. It then counts the number of correct predictiions and outputs the ratio of correct predictions\n",
    "\n",
    "Now lets use the cross_val_score() function using our SDG classifier using K fold cross validation with three folds. Remeber that K fold cross validation means splitting the the training set into K folds (in this case 3), then making predictions and evaluating them on each fold using a model trained on remaining folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d0a785b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95035, 0.96035, 0.9604 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41318701",
   "metadata": {},
   "source": [
    "WOW! above 90% accuracy (the ratio of correct predictions) on all cross validation folds! This looks amazing, but sadly this isn't the case, before we get too excited lets look at a really dumb classifier that simply just classifies everything in this dataset as \"not a 5\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7235f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba7291",
   "metadata": {},
   "source": [
    "Can we guess the models accuracy??? Lets have a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cd23b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91125, 0.90855, 0.90915])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7feaf",
   "metadata": {},
   "source": [
    "Lol thats funny, we have around 90% accuracy by simply just saying everything is a 5. This is becuase only about 10% of the MNIST dataset is a 5. So if we always guess that it is not a 5, we will still get a really high score. \n",
    "\n",
    "This demonstrates why accuracy is generally not a great performance metric for classifiers. Especially when we are dealing with skewed datasets (when one of the classes is much more fequent than the other).\n",
    "\n",
    "### Confusion Matrix \n",
    "\n",
    "A much better way to evaluate the performance of a classifier is the use a confusion matrix. The general idea for the confusion matrix is to count the number of times instances of a class A are classified as class B. For example, to know how many times the classifier confused images of 5s as 3s we would look on the 5th row an the 3rd column of the confusion matrix. \n",
    "\n",
    "To compute the confusion matrix, we first need a set of predictions, so that they can be compared with the actual targets. For this we use the cross_val_predict() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ab0e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_predictions = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c00555",
   "metadata": {},
   "source": [
    "Just like cross_val_score(), cross_val_predict() preforms Kfold cross validations but instead of returning evaluation scores, it returns the predictions made on each test fold. This means that we get a clean predicition for each instance in the training set (the term 'clean' means that the predicition was made by the model on an instance that it has not seen before in training). \n",
    "\n",
    "Now that we are ready to get to the confusion matrix we can get this by using the confusion_matrix() function. Just pass the target class and the predicted class into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5b24562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53892,   687],\n",
       "       [ 1891,  3530]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0029c",
   "metadata": {},
   "source": [
    "Each row in the confusion matrix represents and **actual** class, while each column represents a **predicted** class. The first row of this matrix considers the non-5 images (the negative class). 53892 of them were correctly predicted as negative, while the remaining 687 were wrongly classified as 5s (flase positive) when they actually were not 5s. On the second row is the positive class (images of 5), 1891 were falsely identified as not 5s (false negative) when in fact there were and 3530 images were correctly identified as 5s (true positive) in the cross validation. A perfect classifier would only have true positives and true negatives while having nonzero values on the main diagonal (top left to bottom right). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7f4d1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54579,     0],\n",
       "       [    0,  5421]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_perfect_predicitions = y_train_5 # we are pretending that we have reached perfect predictions\n",
    "confusion_matrix(y_train_5, y_train_perfect_predicitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b38d4",
   "metadata": {},
   "source": [
    "The confusion matrix gives you a lot of information, but sometimes we may prefer a more concise metric. An interesting metric is the **precision** of the classifier, this is the accuracy of positive predicitions, and is defined as:\n",
    "\n",
    "Equation 3.1 Precision\n",
    "\n",
    "precision $= \\frac{TP}{TP + FP}$\n",
    "\n",
    "TP is the true positive rate and FP is the false positive rate. A trivial way to have 100% precision is to make only one prediction and make sure that predicition is correct (precision = 1/1 = 100%). But this would not be very useful would it!!! So typically we use precision in conjunction with **recall**, also called **sensitivty** or the **True positive rate**, this is the ratio of positive instances that are correctly detected by the classifier. \n",
    "\n",
    "Equation 3.2 Recall / True Positive Rate\n",
    "\n",
    "Recall $= \\frac{TP}{TP + FN}$\n",
    "\n",
    "FN is the number of false negatives predictd by the classifier (the 5s that were missclassifed as 3s) \n",
    "\n",
    "### Precision and Recall \n",
    "\n",
    "Scikit-learn provides several functions that allow us computre classifier metrics, including precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "779da069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8370879772350012"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_train_5, y_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8508f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6511713705958311"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train_5, y_train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93bd2c",
   "metadata": {},
   "source": [
    "Now that we have broken our SGD classifier down we can now see it is not as great as first seen. We now can see that when our classifier says its a 5, its only a 5 83% of the time, not the > 90% that we got using accuracy. Moreover, our classifier only picks up 65% of the 5s that are actually in the dataset! Thats not great. \n",
    "\n",
    "It is often convenient when we are comparing classifiers to merge the precision and recall into one metric, this metric is termed the $F_1$ score. The $F_1$ score is the **harmonic mean** of precision and recall (Equation 3.3). Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result the classifier will only get a high $F_1$ score if both recall and precision are high. \n",
    "\n",
    "Equation 3.3 $F_1$ score \n",
    "\n",
    "$F_1$  = $\\frac{2}{\\frac{1}{precision}+\\frac{1}{recall}}$ = $2 \\times \\frac{precision \\times recall}{precision + recall}$ = $\\frac{TP}{TP} + \\frac{FN + FP}{2}$\n",
    "\n",
    "To compute the $F_1$ score, we can simply call the $F_1$ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9edf1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325171197343846"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train_5, y_train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ade104",
   "metadata": {},
   "source": [
    "The $F_1$ score favours classifiers that have similar precision and recall. This is not always what we want however. Some contexts demand that we care more about precision and others demand more recall. If we were training a classifier to filter content for a childerens site. We would favour precision to be high as we would prefere a classifier that rejects many good videos (low recall) but we know that all videos on the website are okay for the child to watch. We might even want to have a human pipeline at the end that double checks the classifier and its decision. Whereas if we were building a security camera software, we would prefer to have a higher recall. This means that the security guards may be altered to some false alarms (false postives and lower precision) but we can be sure that all of the shoplifters are caught. \n",
    "\n",
    "Unfortunately, we cannot have it both ways! Increasing precision of the model reduces recall and increasing recall reduces precision (there is no way we can increase both at the same time). This is called the **precision recall tradeoff**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
