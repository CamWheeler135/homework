{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f745402",
   "metadata": {},
   "source": [
    "# Chapter 2 Hands On Machine Learning Cheet Sheet\n",
    "\n",
    "### Here is a cheet sheet I have made organised into chapters and subheadings as an easy, quick recall of the code adn topics we have covered throughout this book!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df7693",
   "metadata": {},
   "source": [
    "### Data Science Checklist\n",
    "\n",
    "- Look at the big picture \n",
    "- Get the data\n",
    "- Discover and visualise the data to gain insights \n",
    "- Prepare the data for Machine Learning algorithms\n",
    "- Select a model and train it \n",
    "- Fine tune the model\n",
    "- Present your solution \n",
    "- Launch, monitor and maintain our system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e580cf",
   "metadata": {},
   "source": [
    "### Look at the Big Picture\n",
    "\n",
    "- Download the data we are working with\n",
    "    - make a directory \n",
    "    - remeber path to collect and where to put the data\n",
    "    - tarfile.open (for tgz files) \n",
    "    - .close(), closes the function\n",
    "    - pd.read_csv(), reads the csv into a dataframe\n",
    "    \n",
    "- Explore the data \n",
    "    - .info(), gets a quick description of the data\n",
    "    - .value_counts(), helpful with catagorical data, gives ys a count of each of the values in the column\n",
    "    - .describe(), shows us a summary of the numerical attributes \n",
    "    - .hist(bins=x, figsize=y), creates a quick histogram of the data, need to use plt.show() to show the data\n",
    "    - .head(), shows the first 5 indecies of the df\n",
    "    \n",
    "- Create a Test Set\n",
    "    - split_into_train_test_sets(data, test_ratio)\n",
    "    - test_set_check(identifer, test_ratio), allows us to give a unique identifer for each index, we can then split the test train set based on their identifier. If the dataframe doesn't have one, use .reset_index() to create a column using the index column as the identifier\n",
    "    - dataframe['column'] allows us to access a particular column\n",
    "    - dataframe['column'].hist, creates a histogram of the column\n",
    "    - 'stratified sampling', ensures that the data is representative of the overall population\n",
    "    - need to make sure we have sufficient numbers in our stratum with sufficent data in each strata \n",
    "    - we split the dataset base of the most influential attribute into stratum based \n",
    "    - StratifiedShuffleSplit(n_splits, test_size, random_state), splits the test train set to it is representative of the data\n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe629c25",
   "metadata": {},
   "source": [
    "### Discover and Visulaise the Data\n",
    "\n",
    "- Make a copy\n",
    "    - training_set.copy(), allows us to play around with the data without breaking it\n",
    "   \n",
    "- Visualising Data\n",
    "    - dataframe.plot(kind=, x, y) \n",
    "    - add alpha to the parameters changes the colour based on concentration\n",
    "    - worth exploring the .plot() method more\n",
    "   \n",
    "- Coreelation Matrix \n",
    "    - dataframe.corr()\n",
    "    - dataframe['column'].sort_values(ascending=Flase), gives us a list of columns that correlate from the most to the least\n",
    "    - scatter_matrix(dataframe['attributes'], figsize)\n",
    "    - need to set the attributes we want to plot under a variable and pass it in\n",
    "    \n",
    "- Creating new attributes\n",
    "    - dataframe['new attribute column name'] = dataframe['attribute'] (/,*,-,+ for example) dataframe['attribute'] and example would be rooms per household is calculated by dividing the column rooms by the column households\n",
    "    - check the new attributes correlation using .corr() again\n",
    "\n",
    "- Cleaning the data \n",
    "    - remove the labels from the predictors\n",
    "    - .drop('comlumn name', axis=1) \n",
    "    - dataframe['column we want to copy].copy()\n",
    "    \n",
    "    \n",
    "#### Data Cleaning \n",
    "\n",
    "- Correct missing data \n",
    "    - .drop(), drops the data\n",
    "    - fill in the missing data (we can use the median, mean etc) \n",
    "    - SimpleImputer(strategy='median' or whatever we want) \n",
    "    - NEED to use .fit() method to fit the imputer to the data before using it \n",
    "    - This can only happen on numerical data, need to remove the catagorical data before doing this\n",
    "    - can access some values of the imputer through the instance we create off it. imputer.statistics_, imputer.strategy\n",
    "    - imputer.transform(dataframe we want to transfrom) fills in the missing data\n",
    "    - pd.datagrame(transformed df, columns, index) \n",
    "    \n",
    "    \n",
    "#### Scikit_Learn Design\n",
    "\n",
    "- Estimators, estimate some parameters based on a dataset (such as the imputer used above) the imputers strategy is a 'hyperparameter'\n",
    "- Transformers, are estimators that can transform the dataset, this action is performed by the transform() method. We pass the dataset we want to transform as a parameter for this\n",
    "- All transformers have fit_tranform() to make it easier as well\n",
    "- Predictors, are estimators that are capable of making predicitons, (linear regression model for example), a predictor has a predict() method where it takes the dataset of new instances and returns a dataset of predictions. It also has a score() method that measures the quality of the predicition\n",
    "- We can access the hyperparameters of each estimator via their instance variables using the dot notation, and all of the learned parameters are accesable by using the underscore e.g imputer.statistics_ (because it is learned) \n",
    "\n",
    "\n",
    "### Handling Text and Catagorical Attributes\n",
    "\n",
    "- Creating a new datframe for the catagorical attributes\n",
    "    - new name = old data frame[[catagory column we want]]\n",
    "    - .value_counts()\n",
    "    - OrdinalEncoder() takes each catagory and assings it a numerical value (need to make an instance) \n",
    "    - this is a transformer, we use the method fit_transform(catagorical dataframe) \n",
    "    - Issue with this is that the ML algo will think 0 and 1 are closer than 0 and 4 but that is not the case\n",
    "    - OneHotEncoder() fixes this as it converts each name into a vector (this is a transformer so we can use fit_transform(catagorical dataframe)\n",
    "    \n",
    "    \n",
    " \n",
    " ### Custom Transformers\n",
    " \n",
    " - Need to be able to make custom transformers for the nitty gritty clean up operations or combining specific attributes. \n",
    " - For it to work with scikit-learn such as their pipelines, create a class and impliment the three methods, fit() this must return self, transform() and fit_transform()\n",
    " - Get fit_transform() for free by using the TransformerMixin in the base class (parent class), we can also use BaseEstimator will give us get_params() and set_params() methods, these are useful when we are tuning hyperparameters\n",
    " \n",
    " - class CombinedAttributeAdder(BaseEstimator, TransformerMixin):\n",
    " \n",
    " \n",
    " ### Feature Scaling \n",
    " \n",
    " - We need to scale our data, ml algorithms do not like to work with inputs that have very different sizes \n",
    " - Min max scaling - data is shifted to fit between 0 and 1 (we can change this) \n",
    " - MinMaxScaler\n",
    " - Standarisations scaling - subtracts the mean value, divides by standard deviation \n",
    " - StandardScaler\n",
    " \n",
    " \n",
    " ### Transformation Pipelines\n",
    " \n",
    " - scikit-learn provides a Pipeline class that allows us to be used in sitatuions where we will need to take steps in a certain order such as use an imputer, then a transformer, then a scaler.\n",
    " - All of the parts of the pipeline must have the fit_transform() method\n",
    " - The Pipeline constructor takes a list of name/estimator pairs, for sample numerical pipeline = Pipeline(['imputer', SimpleImputer(stratgey=\"median\")), \"attributes_adder\", CombinedAttributesAdder()), 'std_scaler', StandardScaler())])\n",
    " \n",
    "- We then pass the dataset we want to do all of this too into the function all line, dataframe_transfromed = numerical pipeline.fit_transform(dataframe)\n",
    "\n",
    "- We can also handle both catagorical and numerical data. To do this we use the ColumnTransformer, but first we need to set the numerical attributes and the catagorical attributes before using this. \n",
    "- This takes a list of tuples, where each tuple contains a name, a transformer and a list of columns we want to transform.\n",
    "\n",
    "- number attributes = x, catagorical attributes = y \n",
    "- full_pipeline = ColumnTransformer([('num', numerical pipeline, number attributes), ('cat', OneHotEncoder(), catagorical attributes)])\n",
    "- We then create a prepared dataset variable = full pipeline.fit_transform(full dataframe) \n",
    "\n",
    "\n",
    "\n",
    "### Select and Train a Model\n",
    "\n",
    "- create an instance of the model class, lin_reg = LinearRegression()\n",
    "- remeber to fit lin_reg.fit(prepared dataset, labels of the dataset)\n",
    "- then we call predict lin_reg.predict(prepared dataset)\n",
    "- mean_squared_error() allows us to compute the error mse = mean_squared_error(labels, predictions) \n",
    "- np.sqrt(mse), calculated the rootmeansquarederror\n",
    "\n",
    "- when creating a new model, we run the same process, create and instance, fit the model using the prepared_data and the labels, call the predcit function, calculate the error. \n",
    "\n",
    "\n",
    "### Better Evaluation of Model Performance\n",
    "\n",
    "- scikit-learns cross_val_score allows us to compute the cross validation, it takes the training set, splits it into folds and trains the model on all the folds but one, then rotates. \n",
    "- scores = cross_val_scores(model, prepared data, labels, scoring, cv) \n",
    "- Note that cross validation expects a utility function where the greater is better, rather than a cost function where smaller is beyyer, so we need to make the result that is returned negative \n",
    "- model_RMSE = np.sqrt(-scores) \n",
    "\n",
    "- create a display scores function \n",
    "- def display_scores(scores):\n",
    "    print('scores', scores)\n",
    "    print('mean', scores.mean())\n",
    "    print('sd', scores.std())\n",
    "\n",
    "- Then we call the display scores function(model_rmse) \n",
    "\n",
    "- when making a new model, the process is the same. model_scores = cross_val_score(model, prepared data, labels, scoring, cv) \n",
    "- rmse = nq.sqrt(-model_scores)\n",
    "- display scores (rmse)\n",
    "\n",
    "\n",
    "### Fine Tune Your Model\n",
    "\n",
    "- Could do it manually but NOOOOOOOOOO its not what we do in cs\n",
    "- The first option we have is to GridSearchCV\n",
    "\n",
    "- When using this we need to set a params_for_gridsearch = where we feed it dictionaries of {'estimators' : [3,10,30], max features : [2,4,6,8]}, {bootstrap = Flase, 'estimators' : [3,10], 'max features' : [2,3,4]}]\n",
    "- We then make an instance of the Moodel we want to use. \n",
    "- Feed all of this into the gridsearch = GridSearchCV(model, param_grid, cv, scoring, return train score = True) \n",
    "- Fit this to the data, gridsearch.fit(prepared data, labels) \n",
    "\n",
    "- We can access the parameters using the dot notation and _ on the variable instance, grid search.best_params_, grid search.best_estimator_\n",
    "- We can also look at all of them at once using for loops, where we zip the mean test score and the params together from their desired dataframes. \n",
    "\n",
    "- Convert it to a dataframe pd.DataFrame(results from grid search params.cv_results) \n",
    "\n",
    "### Randomised Grid Search\n",
    "\n",
    "- GridSearchCV is good but what if our hyperparameter space is really large? We use RandomisedGridSearch. This randomly throws combinations together instead of trying all of them. \n",
    "\n",
    "\n",
    "### Ensemble Methods \n",
    "\n",
    "- Where we combine several models together, works well if models make differnt types of errors\n",
    "\n",
    "### Analyse the Best Models and Their Errors\n",
    "\n",
    "- We can measure feature importance by using the .feature_importances_ \n",
    "- Do this on top of grid_search.best_estimator_ call\n",
    "\n",
    "\n",
    "### Evaluate Your System on the Test Set\n",
    "\n",
    "- Use the final model which is the .best_estimator_ of the GridSearchCV\n",
    "- Use the x test as all of our values, bar the predictors\n",
    "- y test are our predictors\n",
    "- fully prep the test data\n",
    "- make predictions, final_model.predict(prepped x test data) \n",
    "- final mean squared error(y test, predictions made from the function above) \n",
    "- np.sqrt(fine mean squared error) \n",
    "\n",
    "- If the generalisation error is not high enough to just launch we can compute the 95% confidence interval \n",
    "- confidence = 0.95\n",
    "- np.sqrt(stats.t.interval(confidence, len(squared errors data) -1, loc=squared errors mean(), scale = stats.sem(sqaured_errors))) \n",
    "- Need to research the stats module as this is used alot. \n",
    "\n",
    "\n",
    "- We need to resist the urg to tamper with the hyperparameters on the test set this will not work, it will only make the model fit the test set more. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
