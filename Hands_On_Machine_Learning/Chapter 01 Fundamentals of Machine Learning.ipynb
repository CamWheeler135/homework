{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f694fb1b",
   "metadata": {},
   "source": [
    "# Chapter 01 Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19a894",
   "metadata": {},
   "source": [
    "Where does machine learning start and end? What does it mean for a machine to learn something? In this chapter, we will clarify what machine learning is and why you may want to use it. \n",
    "\n",
    "Before looking at the whole world of machine learning, we will first look at a map and learn the main regions and the most notable landmarks. Supervised vs Unsupervised, online vs batch learning, instance based learning vs model based learning. We will then look at the work flow of a machine learning project, discuss the main challenges we will face and cover how to evaluate and fine tune our machine learning system.\n",
    "\n",
    "This chapter will introduce a lot of fundamental concepts that every data scientist should know by heart (so this chapter is important). It will be high level overview (there isn't much code in this chapter!) but we need to know this so so so so so well before moving on! SO LETS GO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427d83f",
   "metadata": {},
   "source": [
    "### What is Machine Learning? \n",
    "\n",
    "Machine learning is the science (and art) of programming computers to learn from data. \"Machine learning is the feild of study that gives computers the ability to learn without being explicity programmed.\" and \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if it's performance on T, measured by P impoves with experience. \n",
    "\n",
    "Examples that a computer uses to learn is called a training set. Each training example is called a training instance, or sample. \n",
    "\n",
    "An example will be spam filters, where task T is to classify emails as spam or not spam, the experience E is the training data, and the performance P needs to be defined; for example, we can use the ratio of correctly classified emails. This particular perforamce measure is called accuracy and is commonly used in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dee5d",
   "metadata": {},
   "source": [
    "### Why Use Machine Learning? \n",
    "\n",
    "conside how you would write a spam filter using traditional programming techniques. \n",
    "\n",
    "1st we would consider what comes up in spam emails, we might look at phrases such as \"4U, credit card, free, won, amazing\" in the title, as these will often come up in spam emails a lot. Perhaps we would also notice a few patterns in the senders address, the the body of the email and other parts of the email. \n",
    "\n",
    "2nd we would write a detection algorithm for each of the patters that we have noticed, and our program would flag emails as spam if a number of these patterns are detected. \n",
    "\n",
    "3rd we would test our program and repeat the steps 1 and 2 until it is good enough to launch to the public. \n",
    "\n",
    "\n",
    "**Classic Programming**\n",
    "\n",
    "Study the problem, write the rules, evaluate ----> if bad, analyse errors and restart the process.\n",
    "\n",
    "Study the problem, write the rules, evalaute ----> if good, launch to the public.\n",
    "\n",
    "**Machine Learning**\n",
    "\n",
    "Study to problem, train a machine learning algorithm on data, evaluate ----> if bad, analyse errors and rester the process. \n",
    "\n",
    "study the problem, train a machine learning algorithm on data, evauluate ----> if good, launch to the public. \n",
    "\n",
    "\n",
    "Since the problem is difficult, our program will likely become a long list of complex rules that is hard to maintain. In contrast to this, a spam filter based on machine learning is rather simple, the computer will automatically learn what phrases are good predictors of spam by detecting unusually frequent patterns of words in the spam example training set compared to the non-spam emails. This program will be much shorter and easier to maintain and most likely more accurate then the human desinged program. \n",
    "\n",
    "\n",
    "It is also worth thinking that the earth is an ever changing envrionment, what happens if spammers learn that 4U causes emails to be placed into spam folder and start writing 'For U' in the email. If we had programmed the spam detection, we would have to go back to the program and rewrite the code to look for 'For U' instead. \n",
    "\n",
    "In contrast a machine learning algorithm, will automatically notice that 'For U' is appearing in spam emails flagged by users and adapt itself to flag them without our intervention.\n",
    "\n",
    "Another area where machine learning shines is for where problems are either too difficult for traditional programming or if there is no algorithm known. For example in speech recognition. Say we want to start simple and write a program that capabale of distinguishing the words one and two. We could notice that the word two starts with a higher pitch at the start (T), so we could hardcode the an algorithm to measure the sound intensity and use that to distinguish between one and two. But what happens when we want to distingush the words of english? what about languages of the world??????? It would be impossible. \n",
    "\n",
    "Finally machine learning can help humans learn as well! ML algorithms can be inspected to see what they have learned (although some algorithms can be hard to do this, I think we are talking about deep learning!). For instance, once a spam filteer has traoned one enough spam email, it be easily be inspected to reveal the list of words and combinations of words that it has learned to be the best predictors of spam emails. SOMETIMES, this will reveal correlations or new trends in the data that we havn't seen before, therefore leading to new understanding or better understanding of the problem. Applying ML techniques to dig into large amounts of data can help draw conclusions and discover patterns that were not immediately apparent to human analysts. \n",
    "\n",
    "---- To Summerise ---- \n",
    "\n",
    "- Problems for which existing solutions require complex programs, a lot of fine tuning or a long list of rules can benifit from machine learning by simplifying the code and perform better than traditional approaches. \n",
    "- Problems where traditional programming yeilds not so good solutions, the machine learning approach can perhaps find a solution. \n",
    "- Where the environment is fluctuating, a machine learning system can adapt to the new data.\n",
    "- Machine learning can yeild results from complex and large amounts of data that are not easily broken down and analysed by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ae771",
   "metadata": {},
   "source": [
    "### Examples of Applications\n",
    "\n",
    "Lets look at some concrete examples of Machine Learning tasks, along with the techniques that can tackle them. \n",
    "\n",
    "Analyzing images of products on a production line to automatically classify them ---> This is an image classification role and is typically performed using convolutional nerual networks. \n",
    "\n",
    "Detecting tumours in brain scans, this is called sematntic segmentation, where each pixel in the image is classified as we want to determine the exact location and shape of the tumour, typically using CNNs as well! \n",
    "\n",
    "Automatically classify news articles, this task is a natural language processing (NLP) task, more specifically a text classification which can be tackled by recurrent neural networks (RNNs), CNNs or Transformers. \n",
    "\n",
    "Automatically classify offensive or inapropriate comments on discussion forums, this is also a NLP task and uses the same stuff. \n",
    "\n",
    "Creating a chat bot or personal assistant, this inolves many NLP components, including natural language understanding (NLU) and question-answering modules. \n",
    "\n",
    "Forcasting your companys revenue next year based on many performance metrics. This is a regression task (predicting values) that can be tackled using any regression model such as Linear or Polynomial Regression Model, a regression SVM, a regression Random Forest, or an artifical neural network (ANN). If we want to take into account sequences of past performance we might want to use an RNN, CNN or Transformers \n",
    "\n",
    "Representing a complex, high dimensional dataset in a clear and insightful diagram. This is called data visualisation often involving dimensionality reduction technqiues \n",
    "\n",
    "Recccomending a product to a client may be interested in, based on past purchasing history. This is a recommender system. One approach is to feed past purchases of the client into an ANN and get it to output the most likely next purchase. This neural network would typically be trained on past sequences of purchases across all clinents (training set?)\n",
    "\n",
    "Building an intelligent bot for a game, this is often tackled using reinforcement learning, when is a branch of machine learning that trains agents to pick actions based on getting the most reward over time, within a given environment. The famous AlphaGo program that beat the world champion at the game Go was built using RL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4c020",
   "metadata": {},
   "source": [
    "### Types of Machine Learning\n",
    "\n",
    "There are so many types of machine learning it is worth classifying them into broad catagories. Based on certain criteria. \n",
    "\n",
    "- Whether or not they learn via human supervision (supervised, unsupervised, semi-supervised and reinforcement learning. \n",
    "\n",
    "- Whether or not they can learn incrementally on the fly (online learning vs batch learning).\n",
    "\n",
    "- Whether or not they work by comparing new data with known datapoints vs detecting patterns in the training data and building a predictive modle, much like scientists do. \n",
    "\n",
    "These criteria are NOT exclusive, for example a state of the art spam filter may learn on the fly using a deep neural network model that has been trained using examples of spam and normal (ham) emails; this makes it an online, model based supervised learning system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28b77a",
   "metadata": {},
   "source": [
    "### Supervised or Unsupervised Learning\n",
    "\n",
    "We can classify machine learning systems based on the amount and type of supervision they get when they are training. There are four major types, supervised, unsupervised, semi-supervised and reinforcement. \n",
    "\n",
    "\n",
    "**Supervised Learning**\n",
    "\n",
    "In supervised learning, the training set you feed into the algorithm includes the desired solutions (the answers) these are called 'labels'. The most typical type of supervised learning task is classification. The spam filter example is a good example of this: it is trained with labelled examples of what is spam and what isnt spam and it must learn to how to classify new emails. \n",
    "\n",
    "Another typical task is to predict target numeric value, such as the price of a car given a certain set of features (milage, brand, age etc) these are called 'predictors'. This sort of task is called regression (predict a value, given an input feature. There are usually multiple input features and soemtimes multiple output values) to train a system like this for the car we would need to give it many examples of cars, their features and their price!\n",
    "\n",
    "---- Note ----\n",
    "In machine learning an 'attribute' is a data type (e.g milage), while a 'feature' has several meanings depending on the context but generally means an attribute that is paried with its value (e.g milage = 13,000). Many people use the words attribute and feature interchangeably. \n",
    "\n",
    "Also note that some regression algorithms cna be used for classification as well. For example logistic regression is commonly used for classification as it can produe ce an output value that corresponds to the probabilty of belonging to a certain class (e.g 20% chance of bein spam) \n",
    "\n",
    "We will cover some important supervised learning algorithms \n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- Linear Regression \n",
    "- Support Vector Machine \n",
    "- Decision Tress and Random Forests \n",
    "- Neural Networks (although some can be unsupervised such as autoencoders, some can be semi supervised such as deep belief networks) \n",
    "\n",
    "**Unsupervised Learning** \n",
    "\n",
    "In unsupervised learning, the training data is not labelled (unlabelled) and the system tries to learn without a teacher. \n",
    "\n",
    "Here are some of the most common unsupervised learning algorithms that are covered later in the book. \n",
    "\n",
    "**clustering**\n",
    "- K-means \n",
    "- DBSCAN\n",
    "- Hierarchial Cluster Analysis (HCA) \n",
    "\n",
    "**anomaly detection an novelty detection**\n",
    "- One-class SVM \n",
    "- Isolation Forest\n",
    "\n",
    "**Visulisation and Dimensionaltiy Reduction**\n",
    "- Principle Component Analysis (PCA)\n",
    "- Kernal PCA \n",
    "- Locally Linear Embedding (LLE) \n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "**Association Rule Learning** \n",
    "- Apriori \n",
    "- Eclat \n",
    "\n",
    "\n",
    "For example, lets say we have some information based on visitors to a blog, we can run a clustering algorithm that will group visitors into a group of similarities. But at no point do we tell the algorithm which group a visitor belongs too, the algorithm will find the connections without our help. For example the algorithm might notice 40% of our blog visitors are males who love comic books and will read our blog in the evening before bed, while 20% of our visitors young sci-fi lovers who read our blog on the weekend. If we use hierarchical clustering algorithm, it might subdivide those visitors into smaller groups. This can help us target specific audiences with our posts. \n",
    "\n",
    "Visualisation aglorithms are also great examples of unsupervised learning, we feed the algorithm complex high dimensional data and it will output 2D or 3D representation so that it can be easily plotted. These algorithms try to preserve the structure as much as it can (trying to keep separete clusters in the input space from overlapping) so that we can understand how the data is organised and perhaps identify unsuspected patterns. \n",
    "\n",
    "A related task is dimensionality reduction, in which the goal is to simplify the data without losing too much information. One way to do this is to merge several correlated features into one (what I am doing in my disso?). For example a cars milage might strongly correlate to its age, so the dimensionality reduction algorithm will merge them into one feature that represents the cars wear and tear!!! This is called feature extraction. It is often a good idea to feed our data into a dimensionality reduction algorithm before feeding it into another machine learning algorithm such as a supervised learning algorithm. It will run faster, take up less memory and could in some cases perform better. \n",
    "\n",
    "Another important unsupervised task is anomaly detection such as credit card spending, catching manufacturing defects or looking for defective data before feeding it into another machine learning algorithm. The system is shown mostly normal instances during training and it learns to recognise them, we then feed a new instance it can tell weather it looks like a normal instance or an anomaly. A similar task is vovelty detection, this requires we have a very clean dataset that doesn't have an examples of the data we are trying to detect. For example if we have pictures of dogs even if we have one Labrador the algorithm will not say Labrador is a novelty, on the other hand a anomaly detection algorithm might see that there is only 1 Labrador and then detect another Labrador as an anomaly in the data.\n",
    "\n",
    "Finally another common unsupervised learning task is assocation rule learning, in which the goal is to dig into really complext data and find relations between attributes. For example if we own a supermarket, would use an association rule algorithm to find corrections in the sales of our data and find that people who buy BBQ sauce and chips will also buy steak as well. \n",
    "\n",
    "\n",
    "**Semi Supervised Learning** \n",
    "\n",
    "Some algorithms deal with partially labelled data usually a lot of unlabelled data and some small amounts of labelled data, an example of this is the Google photos, where it groups people such as person A is in photo 1,3,7 and person B is in 4,6,2 based on their looks and will only take one label (us telling google who the person is) for google to go and find them in all of the other photos. Although in practice this doesn't work so well, sometimes we need to label a person a couple of times before google starts to get ahold of the photos properly. \n",
    "\n",
    "Most semi supervised learning algorithms are combinations of unsupervised and supervised algorithms, such as deep belief networks (DBP). Thesea are based on unsupervised components called restrited Boltzmnn Machines (RBMs) stacked on top of one another. RBMs are trainned sequentially in an unsupervised manner and then the fine tuning is done using supervised learning techniques. \n",
    "\n",
    "\n",
    "**Reinforcement Learning** \n",
    "\n",
    "\"This is a very different beast\" The learning system called an 'agent' in this context, can observe the environment, select and perform actions and get rewards based on those actions. It must then learn by itself what is the best stratgey called a 'policy' to get the most reward over time. A policy defines what action the agent should choose when it is in a certain situation. \n",
    "\n",
    "For example many robots implement Reinforcement learning algorithms to learn how to walk. DeepMinds AlphaGo program is a great reinforcement learning algorithm. It learned its policy by analysing millions of games, and then playing games against itself. Note that learning was turned off when playing the champion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd1e8e",
   "metadata": {},
   "source": [
    "### Online Vs Batch Learning \n",
    "\n",
    "Another criterion to classify machine learning is wheather or not they system can learn incementally from a stream of data. \n",
    "\n",
    "**Batch Learning/Offline Learning** \n",
    "\n",
    "In batch learning the system is incabale of learning incrementally: it must be trained using all the available data. This will generally take longer and computing resources so it is typically done offline. After training on all of the data it is then lauched into production and runs without learning anymore. This is called 'offline learning'. If we want to update a batch system, we need to train a new model of all of the data (new and old), then stop the old system and replace that old system with the new one. However like mentioned before we can automate the process of updating machine learning systems so it is not as big of a deal as it is made out to be. This solution is simple and will often work fine, but training on a full set of data can take many hours, sometimes even 24 hours plus! so we would typically train a new system only every 24 hours or even just weakly. If our system needs to adapt quickly (such as for stock prices) we need a more reactive solution. \n",
    "\n",
    "However, despite all of this, even if we can automate this process, if we have too much data the computational resources needed to update the system will be very expensive. Furthmore, if the data simply gets to huge, it will be impossible to use a batch learning algorithm. It so also worth concidering the resoruces of our machine, if we are using a smartphone application or a rover on mars, carrying around that much data and taking up a lot of resources to train for hours everyday is not going to work!\n",
    "\n",
    "**Online Learning** \n",
    "\n",
    "In online learning we train the system incrementally by feeding it data instances sequentially, either individually or in small groups named 'mini-batches'. Each learning step if fast and cheap so the system can learn about new data on the fly as it is fed into it. So once we have released it into the public, we keep feeding data to the system so that it can learn continuously. This is great for systems that recieve data as a flow such as stock prices and need to adatp to a rapidlt changing environment. It is also good for systems that have limited computational resources, once an online system has learned about new data instances it does not need them anymore so we can discard them (unless we want to be able to roll back to a previous state and replay the data). This can save a huge amount of space. \n",
    "\n",
    "Out of core learning, is when we train an online learning algorithm of huge data sets that cannot be fit into the memory of the system. The alogrithm will load some of the data, train of that data, then get some more and train again until it runs out of data. \n",
    "\n",
    "---- Note ----\n",
    "\n",
    "Out of core learning is often done offline, i.e it is not done while the system is live! So the online learning name can be a little bit confusing, the book says to think of out of core learning as incremental learning not just \"online learning\" \n",
    "\n",
    "One important parameter of online learning is how fast it can learn and adapt to changing data, this is called the 'learning rate'. If you set a high learning rate, then your system will rapdily adapt to the new data that is being fed into it, but it will also tend to quickly forget the old data (we don't want a spam filter to simply forget the old data and only flag the new kinds of spam it is shown). Conversley, if we set a learning rate too low the system will have more inerita; this is it learns more slowly, but it will also be less senstive to the noise in the new data or to sequences of nonrepresentative data points (outliers). \n",
    "\n",
    "One of the big challenges in online learning is that if it is being fed bad data, the systems performace will gradually decline. If this system is live the perforamce wil decrease and will be noticed by clients. This bad data could be coming from a faulty sensor on a robot, or someone spamming search results in order to get their page to the top result on google. To reduce the risk of this were need to monitor our system closely and promptly switch off learning off the system if we notice a problem. We might also want to monitor the input data and react to abnormal data, such as using an anomaly detection algorithm to pre process the data before feeding it into our online system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a420f",
   "metadata": {},
   "source": [
    "### Instance Based vs Model Based Learning\n",
    "\n",
    "One final way to catagorise machine learning system is how they generalize. Most machine learning tasks are about making predicitons. This means that given a number of training examples, the system needs to be able to make predictions based on examples it has never seen before. Having a good performance metric on the training data is good, but insufficient; the true goal is to perform well on the new instances that the system has not seen before. \n",
    "\n",
    "There are two ways to generalise! Either instanced based learning or model based learning. \n",
    "\n",
    "**Instance Based Learning** \n",
    "\n",
    "The most trivial way to learning is to just learn off by heart. An example of this would be for a spam filter to flag emails that match the training data (emails flagged by users themselves). This is not the worst solution but not the best by far. Another way to do this is to to write a program that looks at emails and flag emails that are similar to spam emails. This requires a measure of similatiry. This could be a count of words that are in common between the suspect email and actual spam emails. The system would flag an email that matches a certain threshold of words in the email. This is isntance based learning!!! The system learns the examples by heart then generalised to new cases by using a similarity measure to compare the new examples with the old examples. So it uses its old training data and compares the new data for similarity classifying it to which the new data is closest. \n",
    "\n",
    "**Model Based Learning**\n",
    "\n",
    "Another way to generalise from a set of examples is to build a model of these examples then use the model to make 'predictors'. This is called model based learning. \n",
    "\n",
    "For example, we want to knwo if money makes people happy, so we download the 'Better Life Index' data from the OECDs website and the stats about gross domesti product (GDP) per captia and the IMFs website. Then you join the tabels and sort by GDP per capita. \n",
    "\n",
    "--- Looking at page 19/20 in the book ---\n",
    "\n",
    "There does seem to be a trend! Although the data is noisy, (partly random) it looks like life satisfaction goes up more or less linearly with GDP increase. So we decide to model life satisfaction as a linear function of GDP per capita. This step is called 'model selection': we selected a 'linear model' of life satisfaction with just one attribute, GDP per capita. \n",
    "\n",
    "Equation 1-1. A simple linear model\n",
    "\n",
    "life_satisfaction = θ0 + θ1 x GDP_per_capita \n",
    "\n",
    "This model has two different parameters, theta 0 and theta 1. By tweaking these parameters we can make our model match any linear function, shown in figure 1.18.\n",
    "\n",
    "Before we can use our model, we need to define the parameter values of theta 0 and theta 1. But how do we know what values will make our model perform the best? To answer this question we need to first specify a perforamnce measure. We can either define a 'utility function' (or fitness function) that measures how good our model is, or we can define a 'cost function' that measures the distance between the linear models prediction and the actual training examples; the objective is the minimise the cost function during this training process. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
