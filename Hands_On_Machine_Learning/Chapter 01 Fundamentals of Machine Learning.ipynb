{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f694fb1b",
   "metadata": {},
   "source": [
    "# Chapter 01 Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19a894",
   "metadata": {},
   "source": [
    "Where does machine learning start and end? What does it mean for a machine to learn something? In this chapter, we will clarify what machine learning is and why you may want to use it. \n",
    "\n",
    "Before looking at the whole world of machine learning, we will first look at a map and learn the main regions and the most notable landmarks. Supervised vs Unsupervised, online vs batch learning, instance based learning vs model based learning. We will then look at the work flow of a machine learning project, discuss the main challenges we will face and cover how to evaluate and fine tune our machine learning system.\n",
    "\n",
    "This chapter will introduce a lot of fundamental concepts that every data scientist should know by heart (so this chapter is important). It will be high level overview (there isn't much code in this chapter!) but we need to know this so so so so so well before moving on! SO LETS GO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427d83f",
   "metadata": {},
   "source": [
    "### What is Machine Learning? \n",
    "\n",
    "Machine learning is the science (and art) of programming computers to learn from data. \"Machine learning is the feild of study that gives computers the ability to learn without being explicity programmed.\" and \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if it's performance on T, measured by P impoves with experience. \n",
    "\n",
    "Examples that a computer uses to learn is called a training set. Each training example is called a training instance, or sample. \n",
    "\n",
    "An example will be spam filters, where task T is to classify emails as spam or not spam, the experience E is the training data, and the performance P needs to be defined; for example, we can use the ratio of correctly classified emails. This particular perforamce measure is called accuracy and is commonly used in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dee5d",
   "metadata": {},
   "source": [
    "### Why Use Machine Learning? \n",
    "\n",
    "conside how you would write a spam filter using traditional programming techniques. \n",
    "\n",
    "1st we would consider what comes up in spam emails, we might look at phrases such as \"4U, credit card, free, won, amazing\" in the title, as these will often come up in spam emails a lot. Perhaps we would also notice a few patterns in the senders address, the the body of the email and other parts of the email. \n",
    "\n",
    "2nd we would write a detection algorithm for each of the patters that we have noticed, and our program would flag emails as spam if a number of these patterns are detected. \n",
    "\n",
    "3rd we would test our program and repeat the steps 1 and 2 until it is good enough to launch to the public. \n",
    "\n",
    "\n",
    "**Classic Programming**\n",
    "\n",
    "Study the problem, write the rules, evaluate ----> if bad, analyse errors and restart the process.\n",
    "\n",
    "Study the problem, write the rules, evalaute ----> if good, launch to the public.\n",
    "\n",
    "**Machine Learning**\n",
    "\n",
    "Study to problem, train a machine learning algorithm on data, evaluate ----> if bad, analyse errors and rester the process. \n",
    "\n",
    "study the problem, train a machine learning algorithm on data, evauluate ----> if good, launch to the public. \n",
    "\n",
    "\n",
    "Since the problem is difficult, our program will likely become a long list of complex rules that is hard to maintain. In contrast to this, a spam filter based on machine learning is rather simple, the computer will automatically learn what phrases are good predictors of spam by detecting unusually frequent patterns of words in the spam example training set compared to the non-spam emails. This program will be much shorter and easier to maintain and most likely more accurate then the human desinged program. \n",
    "\n",
    "\n",
    "It is also worth thinking that the earth is an ever changing envrionment, what happens if spammers learn that 4U causes emails to be placed into spam folder and start writing 'For U' in the email. If we had programmed the spam detection, we would have to go back to the program and rewrite the code to look for 'For U' instead. \n",
    "\n",
    "In contrast a machine learning algorithm, will automatically notice that 'For U' is appearing in spam emails flagged by users and adapt itself to flag them without our intervention.\n",
    "\n",
    "Another area where machine learning shines is for where problems are either too difficult for traditional programming or if there is no algorithm known. For example in speech recognition. Say we want to start simple and write a program that capabale of distinguishing the words one and two. We could notice that the word two starts with a higher pitch at the start (T), so we could hardcode the an algorithm to measure the sound intensity and use that to distinguish between one and two. But what happens when we want to distingush the words of english? what about languages of the world??????? It would be impossible. \n",
    "\n",
    "Finally machine learning can help humans learn as well! ML algorithms can be inspected to see what they have learned (although some algorithms can be hard to do this, I think we are talking about deep learning!). For instance, once a spam filter has trained one enough spam email, it be easily be inspected to reveal the list of words and combinations of words that it has learned to be the best predictors of spam emails. SOMETIMES, this will reveal correlations or new trends in the data that we havn't seen before, therefore leading to new understanding or better understanding of the problem. Applying ML techniques to dig into large amounts of data can help draw conclusions and discover patterns that were not immediately apparent to human analysts. \n",
    "\n",
    "---- To Summerise ---- \n",
    "\n",
    "- Problems for which existing solutions require complex programs, a lot of fine tuning or a long list of rules can benifit from machine learning by simplifying the code and perform better than traditional approaches. \n",
    "- Problems where traditional programming yeilds not so good solutions, the machine learning approach can perhaps find a solution. \n",
    "- Where the environment is fluctuating, a machine learning system can adapt to the new data.\n",
    "- Machine learning can yeild results from complex and large amounts of data that are not easily broken down and analysed by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ae771",
   "metadata": {},
   "source": [
    "### Examples of Applications\n",
    "\n",
    "Lets look at some concrete examples of Machine Learning tasks, along with the techniques that can tackle them. \n",
    "\n",
    "Analyzing images of products on a production line to automatically classify them ---> This is an image classification role and is typically performed using convolutional nerual networks. \n",
    "\n",
    "Detecting tumours in brain scans, this is called sematntic segmentation, where each pixel in the image is classified as we want to determine the exact location and shape of the tumour, typically using CNNs as well! \n",
    "\n",
    "Automatically classify news articles, this task is a natural language processing (NLP) task, more specifically a text classification which can be tackled by recurrent neural networks (RNNs), CNNs or Transformers. \n",
    "\n",
    "Automatically classify offensive or inapropriate comments on discussion forums, this is also a NLP task and uses the same stuff. \n",
    "\n",
    "Creating a chat bot or personal assistant, this inolves many NLP components, including natural language understanding (NLU) and question-answering modules. \n",
    "\n",
    "Forcasting your companys revenue next year based on many performance metrics. This is a regression task (predicting values) that can be tackled using any regression model such as Linear or Polynomial Regression Model, a regression SVM, a regression Random Forest, or an artifical neural network (ANN). If we want to take into account sequences of past performance we might want to use an RNN, CNN or Transformers \n",
    "\n",
    "Representing a complex, high dimensional dataset in a clear and insightful diagram. This is called data visualisation often involving dimensionality reduction technqiues \n",
    "\n",
    "Recccomending a product to a client may be interested in, based on past purchasing history. This is a recommender system. One approach is to feed past purchases of the client into an ANN and get it to output the most likely next purchase. This neural network would typically be trained on past sequences of purchases across all clinents (training set?)\n",
    "\n",
    "Building an intelligent bot for a game, this is often tackled using reinforcement learning, when is a branch of machine learning that trains agents to pick actions based on getting the most reward over time, within a given environment. The famous AlphaGo program that beat the world champion at the game Go was built using RL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4c020",
   "metadata": {},
   "source": [
    "### Types of Machine Learning\n",
    "\n",
    "There are so many types of machine learning it is worth classifying them into broad catagories. Based on certain criteria. \n",
    "\n",
    "- Whether or not they learn via human supervision (supervised, unsupervised, semi-supervised and reinforcement learning. \n",
    "\n",
    "- Whether or not they can learn incrementally on the fly (online learning vs batch learning).\n",
    "\n",
    "- Whether or not they work by comparing new data with known datapoints vs detecting patterns in the training data and building a predictive modle, much like scientists do. \n",
    "\n",
    "These criteria are NOT exclusive, for example a state of the art spam filter may learn on the fly using a deep neural network model that has been trained using examples of spam and normal (ham) emails; this makes it an online, model based supervised learning system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28b77a",
   "metadata": {},
   "source": [
    "### Supervised or Unsupervised Learning\n",
    "\n",
    "We can classify machine learning systems based on the amount and type of supervision they get when they are training. There are four major types, supervised, unsupervised, semi-supervised and reinforcement. \n",
    "\n",
    "\n",
    "**Supervised Learning**\n",
    "\n",
    "In supervised learning, the training set you feed into the algorithm includes the desired solutions (the answers) these are called 'labels'. The most typical type of supervised learning task is classification. The spam filter example is a good example of this: it is trained with labelled examples of what is spam and what isnt spam and it must learn to how to classify new emails. \n",
    "\n",
    "Another typical task is to predict target numeric value, such as the price of a car given a certain set of features (milage, brand, age etc) these are called 'predictors'. This sort of task is called regression (predict a value, given an input feature. There are usually multiple input features and soemtimes multiple output values) to train a system like this for the car we would need to give it many examples of cars, their features and their price!\n",
    "\n",
    "---- Note ----\n",
    "In machine learning an 'attribute' is a data type (e.g milage), while a 'feature' has several meanings depending on the context but generally means an attribute that is paried with its value (e.g milage = 13,000). Many people use the words attribute and feature interchangeably. \n",
    "\n",
    "Also note that some regression algorithms cna be used for classification as well. For example logistic regression is commonly used for classification as it can produe ce an output value that corresponds to the probabilty of belonging to a certain class (e.g 20% chance of bein spam) \n",
    "\n",
    "We will cover some important supervised learning algorithms \n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- Linear Regression \n",
    "- Support Vector Machine \n",
    "- Decision Tress and Random Forests \n",
    "- Neural Networks (although some can be unsupervised such as autoencoders, some can be semi supervised such as deep belief networks) \n",
    "\n",
    "**Unsupervised Learning** \n",
    "\n",
    "In unsupervised learning, the training data is not labelled (unlabelled) and the system tries to learn without a teacher. \n",
    "\n",
    "Here are some of the most common unsupervised learning algorithms that are covered later in the book. \n",
    "\n",
    "**clustering**\n",
    "- K-means \n",
    "- DBSCAN\n",
    "- Hierarchial Cluster Analysis (HCA) \n",
    "\n",
    "**anomaly detection an novelty detection**\n",
    "- One-class SVM \n",
    "- Isolation Forest\n",
    "\n",
    "**Visulisation and Dimensionaltiy Reduction**\n",
    "- Principle Component Analysis (PCA)\n",
    "- Kernal PCA \n",
    "- Locally Linear Embedding (LLE) \n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "**Association Rule Learning** \n",
    "- Apriori \n",
    "- Eclat \n",
    "\n",
    "\n",
    "For example, lets say we have some information based on visitors to a blog, we can run a clustering algorithm that will group visitors into a group of similarities. But at no point do we tell the algorithm which group a visitor belongs too, the algorithm will find the connections without our help. For example the algorithm might notice 40% of our blog visitors are males who love comic books and will read our blog in the evening before bed, while 20% of our visitors young sci-fi lovers who read our blog on the weekend. If we use hierarchical clustering algorithm, it might subdivide those visitors into smaller groups. This can help us target specific audiences with our posts. \n",
    "\n",
    "Visualisation aglorithms are also great examples of unsupervised learning, we feed the algorithm complex high dimensional data and it will output 2D or 3D representation so that it can be easily plotted. These algorithms try to preserve the structure as much as it can (trying to keep separete clusters in the input space from overlapping) so that we can understand how the data is organised and perhaps identify unsuspected patterns. \n",
    "\n",
    "A related task is dimensionality reduction, in which the goal is to simplify the data without losing too much information. One way to do this is to merge several correlated features into one (what I am doing in my disso?). For example a cars milage might strongly correlate to its age, so the dimensionality reduction algorithm will merge them into one feature that represents the cars wear and tear!!! This is called feature extraction. It is often a good idea to feed our data into a dimensionality reduction algorithm before feeding it into another machine learning algorithm such as a supervised learning algorithm. It will run faster, take up less memory and could in some cases perform better. \n",
    "\n",
    "Another important unsupervised task is anomaly detection such as credit card spending, catching manufacturing defects or looking for defective data before feeding it into another machine learning algorithm. The system is shown mostly normal instances during training and it learns to recognise them, we then feed a new instance it can tell weather it looks like a normal instance or an anomaly. A similar task is vovelty detection, this requires we have a very clean dataset that doesn't have an examples of the data we are trying to detect. For example if we have pictures of dogs even if we have one Labrador the algorithm will not say Labrador is a novelty, on the other hand a anomaly detection algorithm might see that there is only 1 Labrador and then detect another Labrador as an anomaly in the data.\n",
    "\n",
    "Finally another common unsupervised learning task is assocation rule learning, in which the goal is to dig into really complext data and find relations between attributes. For example if we own a supermarket, would use an association rule algorithm to find corrections in the sales of our data and find that people who buy BBQ sauce and chips will also buy steak as well. \n",
    "\n",
    "\n",
    "**Semi Supervised Learning** \n",
    "\n",
    "Some algorithms deal with partially labelled data usually a lot of unlabelled data and some small amounts of labelled data, an example of this is the Google photos, where it groups people such as person A is in photo 1,3,7 and person B is in 4,6,2 based on their looks and will only take one label (us telling google who the person is) for google to go and find them in all of the other photos. Although in practice this doesn't work so well, sometimes we need to label a person a couple of times before google starts to get ahold of the photos properly. \n",
    "\n",
    "Most semi supervised learning algorithms are combinations of unsupervised and supervised algorithms, such as deep belief networks (DBP). Thesea are based on unsupervised components called restrited Boltzmnn Machines (RBMs) stacked on top of one another. RBMs are trainned sequentially in an unsupervised manner and then the fine tuning is done using supervised learning techniques. \n",
    "\n",
    "\n",
    "**Reinforcement Learning** \n",
    "\n",
    "\"This is a very different beast\" The learning system called an 'agent' in this context, can observe the environment, select and perform actions and get rewards based on those actions. It must then learn by itself what is the best stratgey called a 'policy' to get the most reward over time. A policy defines what action the agent should choose when it is in a certain situation. \n",
    "\n",
    "For example many robots implement Reinforcement learning algorithms to learn how to walk. DeepMinds AlphaGo program is a great reinforcement learning algorithm. It learned its policy by analysing millions of games, and then playing games against itself. Note that learning was turned off when playing the champion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd1e8e",
   "metadata": {},
   "source": [
    "### Online Vs Batch Learning \n",
    "\n",
    "Another criterion to classify machine learning is wheather or not they system can learn incementally from a stream of data. \n",
    "\n",
    "**Batch Learning/Offline Learning** \n",
    "\n",
    "In batch learning the system is incabale of learning incrementally: it must be trained using all the available data. This will generally take longer and computing resources so it is typically done offline. After training on all of the data it is then lauched into production and runs without learning anymore. This is called 'offline learning'. If we want to update a batch system, we need to train a new model of all of the data (new and old), then stop the old system and replace that old system with the new one. However like mentioned before we can automate the process of updating machine learning systems so it is not as big of a deal as it is made out to be. This solution is simple and will often work fine, but training on a full set of data can take many hours, sometimes even 24 hours plus! so we would typically train a new system only every 24 hours or even just weakly. If our system needs to adapt quickly (such as for stock prices) we need a more reactive solution. \n",
    "\n",
    "However, despite all of this, even if we can automate this process, if we have too much data the computational resources needed to update the system will be very expensive. Furthmore, if the data simply gets to huge, it will be impossible to use a batch learning algorithm. It so also worth concidering the resoruces of our machine, if we are using a smartphone application or a rover on mars, carrying around that much data and taking up a lot of resources to train for hours everyday is not going to work!\n",
    "\n",
    "**Online Learning** \n",
    "\n",
    "In online learning we train the system incrementally by feeding it data instances sequentially, either individually or in small groups named 'mini-batches'. Each learning step if fast and cheap so the system can learn about new data on the fly as it is fed into it. So once we have released it into the public, we keep feeding data to the system so that it can learn continuously. This is great for systems that recieve data as a flow such as stock prices and need to adatp to a rapidlt changing environment. It is also good for systems that have limited computational resources, once an online system has learned about new data instances it does not need them anymore so we can discard them (unless we want to be able to roll back to a previous state and replay the data). This can save a huge amount of space. \n",
    "\n",
    "Out of core learning, is when we train an online learning algorithm of huge data sets that cannot be fit into the memory of the system. The alogrithm will load some of the data, train of that data, then get some more and train again until it runs out of data. \n",
    "\n",
    "---- Note ----\n",
    "\n",
    "Out of core learning is often done offline, i.e it is not done while the system is live! So the online learning name can be a little bit confusing, the book says to think of out of core learning as incremental learning not just \"online learning\" \n",
    "\n",
    "One important parameter of online learning is how fast it can learn and adapt to changing data, this is called the 'learning rate'. If you set a high learning rate, then your system will rapdily adapt to the new data that is being fed into it, but it will also tend to quickly forget the old data (we don't want a spam filter to simply forget the old data and only flag the new kinds of spam it is shown). Conversley, if we set a learning rate too low the system will have more inerita; this is it learns more slowly, but it will also be less senstive to the noise in the new data or to sequences of nonrepresentative data points (outliers). \n",
    "\n",
    "One of the big challenges in online learning is that if it is being fed bad data, the systems performace will gradually decline. If this system is live the perforamce wil decrease and will be noticed by clients. This bad data could be coming from a faulty sensor on a robot, or someone spamming search results in order to get their page to the top result on google. To reduce the risk of this were need to monitor our system closely and promptly switch off learning off the system if we notice a problem. We might also want to monitor the input data and react to abnormal data, such as using an anomaly detection algorithm to pre process the data before feeding it into our online system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a420f",
   "metadata": {},
   "source": [
    "### Instance Based vs Model Based Learning\n",
    "\n",
    "One final way to catagorise machine learning system is how they generalize. Most machine learning tasks are about making predicitons. This means that given a number of training examples, the system needs to be able to make predictions based on examples it has never seen before. Having a good performance metric on the training data is good, but insufficient; the true goal is to perform well on the new instances that the system has not seen before. \n",
    "\n",
    "There are two ways to generalise! Either instanced based learning or model based learning. \n",
    "\n",
    "**Instance Based Learning** \n",
    "\n",
    "The most trivial way to learning is to just learn off by heart. An example of this would be for a spam filter to flag emails that match the training data (emails flagged by users themselves). This is not the worst solution but not the best by far. Another way to do this is to to write a program that looks at emails and flag emails that are similar to spam emails. This requires a measure of similatiry. This could be a count of words that are in common between the suspect email and actual spam emails. The system would flag an email that matches a certain threshold of words in the email. This is isntance based learning!!! The system learns the examples by heart then generalised to new cases by using a similarity measure to compare the new examples with the old examples. So it uses its old training data and compares the new data for similarity classifying it to which the new data is closest. \n",
    "\n",
    "**Model Based Learning**\n",
    "\n",
    "Another way to generalise from a set of examples is to build a model of these examples then use the model to make 'predictors'. This is called model based learning. \n",
    "\n",
    "For example, we want to knwo if money makes people happy, so we download the 'Better Life Index' data from the OECDs website and the stats about gross domesti product (GDP) per captia and the IMFs website. Then you join the tabels and sort by GDP per capita. \n",
    "\n",
    "--- Looking at page 19/20 in the book ---\n",
    "\n",
    "There does seem to be a trend! Although the data is noisy, (partly random) it looks like life satisfaction goes up more or less linearly with GDP increase. So we decide to model life satisfaction as a linear function of GDP per capita. This step is called 'model selection': we selected a 'linear model' of life satisfaction with just one attribute, GDP per capita. \n",
    "\n",
    "Equation 1-1. A simple linear model\n",
    "\n",
    "life_satisfaction = θ0 + θ1 x GDP_per_capita \n",
    "\n",
    "This model has two different parameters, theta 0 and theta 1. By tweaking these parameters we can make our model match any linear function, shown in figure 1.18.\n",
    "\n",
    "Before we can use our model, we need to define the parameter values of theta 0 and theta 1. But how do we know what values will make our model perform the best? To answer this question we need to first specify a perforamnce measure. We can either define a 'utility function' (or fitness function) that measures how good our model is, or we can define a 'cost function' that measures the distance between the linear models prediction and the actual training examples; the objective is the minimise the cost function during this training process. For linear regression problems people ususally use a cost function that measures teh distance between the linear model prediction and the training examples. \n",
    "\n",
    "This is where the Linear Regression algorithm comes in: we can feed it our training data and it finds the parameters that make the linear model fit best to our data. This is called a training model. In our case the alrogithm find that the optimal parameter values are θ0 = 4.85 and θ1 = 4.91x10 to the power of -5. \n",
    "\n",
    "---- Note ----\n",
    "\n",
    "Rather confusingly the same word \"model\" can refer to a type of model we are using (such as linear regression), to a fully specified model architecture (Linear regression with one input and one output) or finally the fully trained model ready to be used for predictions (linear regression model with one input and one output using θ0 = 4.85 and θ1 = 4.91 x 10 to the power of -5. 'Model selection' consists in choosing the type of model and fully specifying its architecture. 'Model training' means running an algorithm to find the model parameters that will make it best fit the training data and then hopefully make good predictions on new data. \n",
    "\n",
    "Now the model fits the training data as closely as possible (for a linear model) as you can see on page 21 figure 1.19. \n",
    "\n",
    "We are now ready to run the model to make predictons, for example lets say we want to know how happy the people of Cyprus are but the dataset doesn't have that data on it. Thats great! we have a model that can help us come up with an answer! We look up Cyprus's GDP per catpita and find 22, 587 dollars as their GDP and then apply our model and find that its life satisfaction is likely to be somewhere between 4.85 + 22,587 x 4.91 x 10 to the -5 = 5.96. \n",
    "\n",
    "To whet our appetite for machine learning. Here is some code below that loads the data, prepares it, creates a scatterplot for visualisation and then trains a linear model and makes a prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554e5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5cb3712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Indicator</th>\n",
       "      <th>Air pollution</th>\n",
       "      <th>Dwellings without basic facilities</th>\n",
       "      <th>Educational attainment</th>\n",
       "      <th>Employees working very long hours</th>\n",
       "      <th>Employment rate</th>\n",
       "      <th>Feeling safe walking alone at night</th>\n",
       "      <th>Homicide rate</th>\n",
       "      <th>Household net adjusted disposable income</th>\n",
       "      <th>Household net wealth</th>\n",
       "      <th>Housing expenditure</th>\n",
       "      <th>...</th>\n",
       "      <th>Personal earnings</th>\n",
       "      <th>Quality of support network</th>\n",
       "      <th>Rooms per person</th>\n",
       "      <th>Self-reported health</th>\n",
       "      <th>Stakeholder engagement for developing regulations</th>\n",
       "      <th>Student skills</th>\n",
       "      <th>Time devoted to leisure and personal care</th>\n",
       "      <th>Voter turnout</th>\n",
       "      <th>Water quality</th>\n",
       "      <th>Years in education</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>37433.0</td>\n",
       "      <td>528768.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>...</td>\n",
       "      <td>55206.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>499.0</td>\n",
       "      <td>14.36</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austria</th>\n",
       "      <td>12.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>86.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>37001.0</td>\n",
       "      <td>309637.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>...</td>\n",
       "      <td>53132.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>491.0</td>\n",
       "      <td>14.51</td>\n",
       "      <td>76.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Indicator  Air pollution  Dwellings without basic facilities  \\\n",
       "Country                                                        \n",
       "Australia            6.7                                 NaN   \n",
       "Austria             12.2                                 0.8   \n",
       "\n",
       "Indicator  Educational attainment  Employees working very long hours  \\\n",
       "Country                                                                \n",
       "Australia                    84.0                               12.5   \n",
       "Austria                      86.0                                5.3   \n",
       "\n",
       "Indicator  Employment rate  Feeling safe walking alone at night  \\\n",
       "Country                                                           \n",
       "Australia             73.0                                 67.0   \n",
       "Austria               72.0                                 86.0   \n",
       "\n",
       "Indicator  Homicide rate  Household net adjusted disposable income  \\\n",
       "Country                                                              \n",
       "Australia            0.9                                   37433.0   \n",
       "Austria              0.5                                   37001.0   \n",
       "\n",
       "Indicator  Household net wealth  Housing expenditure  ...  Personal earnings  \\\n",
       "Country                                               ...                      \n",
       "Australia              528768.0                 19.4  ...            55206.0   \n",
       "Austria                309637.0                 20.8  ...            53132.0   \n",
       "\n",
       "Indicator  Quality of support network  Rooms per person  Self-reported health  \\\n",
       "Country                                                                         \n",
       "Australia                        93.0               NaN                  85.0   \n",
       "Austria                          92.0               1.6                  71.0   \n",
       "\n",
       "Indicator  Stakeholder engagement for developing regulations  Student skills  \\\n",
       "Country                                                                        \n",
       "Australia                                                2.7           499.0   \n",
       "Austria                                                  1.3           491.0   \n",
       "\n",
       "Indicator  Time devoted to leisure and personal care  Voter turnout  \\\n",
       "Country                                                               \n",
       "Australia                                      14.36           92.0   \n",
       "Austria                                        14.51           76.0   \n",
       "\n",
       "Indicator  Water quality  Years in education  \n",
       "Country                                       \n",
       "Australia           92.0                20.0  \n",
       "Austria             92.0                17.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data into a dataframe for the better life index\n",
    "oecd_bli = pd.read_csv(\"data/BLI_11062022213453762.csv\", thousands=',')\n",
    "# aletring the oecd dataset\n",
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "# .pivot returns a dataframe with the desired index and columns\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "\n",
    "oecd_bli.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f3f274",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['Country'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# altering the GDP dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m gdp_per_capita\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDP per capita\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mgdp_per_capita\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m gdp_per_capita\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:5494\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5491\u001b[0m                 missing\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m   5493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 5494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are in the columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   5497\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['Country'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# Loading the data into a dataframe for the GDP\n",
    "gdp_per_capita = pd.read_csv(\"data/WEOApr2022all.xls\", thousands=',', delimiter='\\t',\n",
    "                            encoding='latin1', na_values='n/a')\n",
    "# altering the GDP dataset\n",
    "gdp_per_capita.rename(columns={\"2020\": \"GDP per capita\"}, inplace=True)\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "gdp_per_capita.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6825f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country\n",
       "Australia    7.1\n",
       "Austria      7.2\n",
       "Belgium      6.8\n",
       "Brazil       6.1\n",
       "Canada       7.0\n",
       "Name: Life satisfaction, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oecd_bli[\"Life satisfaction\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_per_capita = pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc72355",
   "metadata": {},
   "source": [
    "### Main Challenges for Machine Learning \n",
    "\n",
    "As a machine learning engineer, our main task is to select a learning algorithm, train it on some and then use it. But two things can go wrong when this happens, there is 'bad algorithm' and 'bad data'.\n",
    "\n",
    "#### Bad Data\n",
    "\n",
    "If we want to teach a human how to pick up an apple, we can simply show them the apple, then show them how to pick it up (repeating these steps until they learn), the person is now able to understand what an apple is (not matter what the colour or shape really, and pick it up). Machine learning is not there yet! It takes a lot of data for machine learning algorithms to work properly, even for the simplest of problems we need lots and lots of data. For complex problems we might need millions of datapoints before we get anywhere. \n",
    "\n",
    "\n",
    "#### The Unreasonable Effectiveness of Data\n",
    "\n",
    "This paper published by microsoft researchers states that even the more simpler learning algorithms perform well given ample datapoints on rather complex tasks such as natural langaue disambiguation. \"These results suggest we may want to consider the trade off between spending more money and time on algorithm development and instead look at spending it on corpus development\" (real world data). However it is worth noting that medium to small datasets will always exisit and it is not cheap to just simply get more data, this takes time and money. \n",
    "\n",
    "#### Nonrepresentative Training Data\n",
    "\n",
    "In order to generalise well it is important that our training data is representative of the new cases (disso!!!). This is the case whether we are using instance based or model based learning methods. If we have training data that is not fully representative of the data we are trying to predict, we will end up with a model that is incorrectly going to predict where or what the new data is. It is unlikely to make accurate predictions. It is cruical to use a training set that is representative of the cases we want to generalise too, this is often harder than it sounds... If the sample is too small we will have 'sampling noise' meaning the data is non repersentative as a result of chance but even if the data is very large we can still get problems of nonrepresentative data if the sampling method is flawed. This is called 'sampling bias'.\n",
    "\n",
    "#### Examples of Sampling Bias\n",
    "\n",
    "An example of sampking bias is the polling for the US president election by the Literary Digest, they send our millions of polls and came to the conclusion that a certain candidate was going to win, however the other candidate won with 62% of the votes. This is because the sampling methodology was flawed. They got addresses from phone directories, magazine subscriptions and club memberships, this favoured the wealthier population that will at large vote republican. As well as this they also did not take into account the people who didn't reply to the poll, becuase they do not care about politics (but still would have voted) or do not like the Literay Digest, as well as other key demographic groups. \n",
    "\n",
    "Another example is that we want to build a dataset of funk music so that we can train our model to classify funk music, we go on youtube and type in funk music, what comes up in the UK will NOT be the same as what comes up in brazil for example, as the type of funk music changes with culture. \n",
    "\n",
    "#### Poor Quality Data\n",
    "\n",
    "If our data is full or error, outliers and noise, it is highly likely that our model will perform pooly as it is much harder for the model to find underlying patterns in our data. It is well worth the time to clean the data and make sure it is ready for analysis. Here are some things we can do.\n",
    "\n",
    "If some instances are clearly outliers it may help if we just simply remove them, or try to fix them manually. If some customers did not fill out their age on the survey (lets say that 5% didn't) we need to decide what to do with that data, we could remove it, fill it in with the median value or train one model with the data and one model without and compare. \n",
    "\n",
    "\n",
    "#### Irrelevent Features\n",
    "\n",
    "Garbage in garbage out, if we do not feed our machine learning model good data, we cannot expect it to give us good results in return. A critical process of machine learning is coming up with a good set of features to train on, This process is called feature engineering and involes the following steps\n",
    "\n",
    "- Feature selection (selecting the most useful features to train on among the existing features) \n",
    "- Feature extraction (combining new features to make a new more useful one) \n",
    "- Creating new features by gathering new data\n",
    "\n",
    "\n",
    "#### Bad Algorithms\n",
    "\n",
    "Now that we have explored what bad data is lets explore the bad algorithm section\n",
    "\n",
    "#### Overfitting the Training Data \n",
    "\n",
    "If we visit another country and the taxi driver rips us off, we as humans might be tempted to jump to the conclusion that all taxi drivers will rip us off in the future, as humans we do this all to often, and machines can fall into the same trap. In machine learning this is called overfitting and it means that the modle performs really well on the training data but does not generalise well at all. Complex models such as deep neural networks can detect subtle patterns in the data and if there is significant noise or the dataset is too small, it can often start to pick up patterns in the noise too. These patterns will not generalise to new data which will result in poorer performance. For example with the satisfaction data mentioneda above, a really complex model could pick up on the fact that countries with a w in have a large chance of being higher in the satisfaction index, but does that actaully relate? No. But the model cannot differentiate between chance and actual patterns so we need to be able to control this. \n",
    "\n",
    "Overfitting happens if a model is too complex relativer to the amount of data and the amount of noise in that data. We can simplify the model by selecting one with fewer parameters, or we can gather more data, or reduce the noise in the training data through feature engineering. \n",
    "\n",
    "We can also constrain a model through a process called 'regularisation', for exmaple a linear regression model has two paramters it changes in order to make the best prediction from the data it is trained on. This gives it two degrees of freedom, is we were to say theta1 has to equal theta0, what we are doing is halfing the alterations it can make, making it much harder to find a perfect fit for the training data. Or more realistically we can set the limit in which the alogrithm can alter theta 1, setting it at a small value, which means the algorithm can only change it a certain amount in order to fit the data. This will produce a model that is simpler than having two totally free degrees of freedom but more complex than a model that only has one degree of freedom. \n",
    "\n",
    "The amount of regularisation to apply during learning can be controlled by 'hyperparameters'. A hyperparameter is a parameter of a learning algorithm (not the model) and as such it is not affected by the learning algorithm itself. But it must be set prior and remain constant throughout training. If we set the regularisation hyperparameter to be very large we will get a flat model where the learning algorithm will certainly not overfit the data but also will not find a good solution. Tuning hyperparameters is a key part of machine learning and we will cover it in depth later on.\n",
    "\n",
    "\n",
    "#### Underfitting the Training Data\n",
    "\n",
    "Underfitting is the opposite of overfitting, it occurs when your model is too simple to learn the underlying patterns in the data. For example a linear model is prone to underfit as it is likely that reality in the real world is just so much more complex than a line, so predictions are bound to be inaccurate even on the training examples. \n",
    "\n",
    "There are several ways we can combat underfitting, we can select a more complex model, we can feed the learning algorithm better features through feature engineering or we can reduce the constraints that have been placed onto the model by reducing the regularisation hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
